{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa7d238",
   "metadata": {},
   "source": [
    "## Window에 bert 설치하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c645e",
   "metadata": {},
   "source": [
    "0. python 가상환경 만들기 --> python -m venv 가상환경이름\n",
    "\n",
    "1. 깃 클론 진행 -> 이때, 설치 경로 상에 한글이 없도록 하기\n",
    "```\n",
    "git clone https://github.com/SKTBrain/KoBERT.git\n",
    "cd KoBERT\n",
    "```\n",
    "\n",
    "2. 다음의 주소를 참고해서 의존성 처리하기\n",
    "https://github.com/SKTBrain/KoBERT/issues/80\n",
    "- 일단, mxnet을 제외한 나머지 다운\n",
    "- 이후, mxnet을 별도로 다운\n",
    "- kobert 내부 py를 바깥으로 복사하기\n",
    "- pip install numpy==1.23진행\n",
    "\n",
    "3. python pytorch_kobert를 진행하기\n",
    "\n",
    "4. import 해서 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd718f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from kobert.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dadd26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d8f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\attention\\code\\GJ\\local_test\\KoBERT\\.cache\\kobert_v1.zip\n",
      "using cached model. C:\\attention\\code\\GJ\\local_test\\KoBERT\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f165b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba59e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5b0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"sample_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbaef9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list()\n",
    "for sen, lab in zip(data[\"0\"], data[\"1\"]):\n",
    "    data_list.append([sen,lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9c2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['부랴부랴 왔는데 아무도 안왔네. 시간개념들이 없네', 2],\n",
       " ['맞아. 사람들이 진짜 개념이없다', 2],\n",
       " ['저렇게는 살지 말아야지', 2],\n",
       " ['그러게 게으르고 멍청한 사람들은 맞아야해', 2],\n",
       " ['인방 보는 남자는 거르는게 맞다', 2],\n",
       " ['특히 벗방보는 애들은 진짜 거세 시켜야함', 3],\n",
       " ['볼 게 없어서 벗방을 보냐 ㅋㅋㅋ', 3],\n",
       " ['사회성이 얼마나 떨어지면 그러냐 ㅋㅋㅋ', 2],\n",
       " ['댓글에 빠순이들 몰려와서 즈그 주인님 쉴드치는 꼴 좀 봐', 2],\n",
       " ['이래서 인방충~ 인방충~ 하는거 구나', 2]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abadf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "351d2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data_list, test_size=0.25, random_state=0) # train : test = 4 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a42c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49707, 16570)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4b520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "\n",
    "        # sentence , label data를 BERT의 입력값에 맞게 변환하는 transformer를 생성\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "\n",
    "        ## 생성한 transformer로 sentence를 변환하여 저장\n",
    "        self.sentences = [transform([data[sent_idx]]) for data in dataset]\n",
    "        self.labels = [np.int32(data[label_idx]) for data in dataset]\n",
    "\n",
    "    def __getitem__ (self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], )) # 각 index에 맞는 item 반환 진행 --> 왜 이런 형태인지는 잘 모르겠음\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf595b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting 진행\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fbe7541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\attention\\code\\GJ\\local_test\\KoBERT\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([   2, 4297, 5940, 2918, 6183, 5439, 3075, 5850, 2423, 7782, 7343,\n",
       "        5937, 3466, 1706, 6060, 5330,  633,    3,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " array(18),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kobert 모듈에서 제공하는 get_tokenizer와 vocab를 활용해 tokneizer를 구성한다\n",
    "tokenizer = get_tokenizer() \n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "data_train = BERTDataset(train_set, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(test_set, 0, 1, tok, max_len, True, False)\n",
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e301bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch에 맞게 최종 입력 dataset 구성 \n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0) ## numworkers 0으로 변경\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a3d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size = 768, num_classes=4, dr_rate=None, params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        ## classifier는 선형 회귀 모델로 구성 (input size = 768, output size = 4 (label이 4개로 구성))\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        ## overfitting 방지를 위한 dropout 비율 설정\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    # attention mask sequence를 구성해주는 함수 --> padding이 아닌 영역을 0에서 1로 변경\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i,v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "\n",
    "        return attention_mask.float()\n",
    "\n",
    "    # bert + classifier를 관통하는 forward 연산 진행\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "\n",
    "        # attention_mask 계산\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        # bert에 input 투입, 변수명이 pooler인거 보니 출력 embedding에 mean pooling 적용한 값이지 않을까 추측\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "\n",
    "        # dropout 비율이 존재한다면, dropout 적용\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "\n",
    "        # classifier 진행\n",
    "        return self.classifier(out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5167b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21efe96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x174ee15fa00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kobert + classifier 불러오기\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.3).to(device)\n",
    "\n",
    "# optimizer (Adam), scheduler 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccab5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_sentence):\n",
    "\n",
    "    # 1. data set 구성 (문장, 라벨)\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    # 2. data를 bert의 입력에 맞게 변환하기\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        # 모델 forward 연산 진행\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        # torch out -> numpy 형식으로 변환\n",
    "        test_eval=[]\n",
    "        logits = out[0].detach().cpu().numpy()\n",
    "        \n",
    "        # 값이 가장 큰 인덱스를 출력\n",
    "        if np.argmax(logits) == 0:\n",
    "            test_eval= \"일반\"\n",
    "        elif np.argmax(logits) == 1:\n",
    "            test_eval = \"욕설\"\n",
    "        elif np.argmax(logits) == 2:\n",
    "            test_eval = \"폭언\"\n",
    "        elif np.argmax(logits) == 3:\n",
    "            test_eval = \"성희롱\"\n",
    "\n",
    "        print(\">> \" + test_eval + \" 문장입니다.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for e in range(1):\n",
    "\n",
    "        train_acc = 0.0\n",
    "        test_acc = 0.0\n",
    "        model.train()\n",
    "\n",
    "        # train set\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            # forward 연산 진행\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "            # loss 는 CrossEntropyLoss를 이용 -> backpropagation 학습 진행\n",
    "            loss = loss_fn(out, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            # train data set 정확도 확인\n",
    "            train_acc += calc_accuracy(out, label)\n",
    "            if batch_id % log_interval == 0:\n",
    "                print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "        print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # test set\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "            # test set 정확도 확인\n",
    "            test_acc += calc_accuracy(out, label)\n",
    "        print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e0455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhksw\\AppData\\Local\\Temp\\ipykernel_6964\\2773772944.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f41b3db8d840d08c969a5aaaae852d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/777 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.3982808589935303 train acc 0.15625\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479395ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
