{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 준비"
      ],
      "metadata": {
        "id": "LRjmrjzLQrAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7WA8hm6Ri_B",
        "outputId": "142b0c1c-7c7d-468c-aeff-6d1892d6cc81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/drive/MyDrive/종합설계/.cache/kobert_v1.zip\n",
            "using cached model. /content/drive/MyDrive/종합설계/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 준비"
      ],
      "metadata": {
        "id": "mKCWgdN8Rqpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"training_dataset_ver3.csv\")\n",
        "\n",
        "data_list = list()\n",
        "for sen, lab in zip(data[\"0\"], data[\"1\"]):\n",
        "  data_list.append([sen,lab])"
      ],
      "metadata": {
        "id": "r3wqar-oRorR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(data_list, test_size=0.1, random_state=0) # train : test = 9:1\n",
        "train_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owz2t4gKR1hJ",
        "outputId": "626945ae-856d-4185-bcb8-ee4c44b45289"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['그 동안 보빨러들이 얼마나 잘 해줬겠어ㅋㅋㅋ', 2]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abuse = pd.read_csv(\"폭언사전.csv\")\n",
        "sexual = pd.read_csv(\"성희롱사전.csv\")"
      ],
      "metadata": {
        "id": "QnQD-ZnpR3XJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abuse_list = list()\n",
        "sexual_list = list()\n",
        "\n",
        "for rows in abuse[\"0\"]:\n",
        "  abuse_list.append(rows)\n",
        "\n",
        "for rows in sexual[\"0\"]:\n",
        "  sexual_list.append(rows)"
      ],
      "metadata": {
        "id": "T1C-GNJuR6UA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abuse_list[:3], sexual_list[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVAhVCHsR8Dt",
        "outputId": "0a47da31-a88a-4343-c8b3-9fc67fc3b4cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['가난', '가난뱅이', '가두'], ['19금', '69', '69자세'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 코버트 다중 분류기 로드"
      ],
      "metadata": {
        "id": "IOFUSEARR_dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "\n",
        "    # sentence , label data를 BERT의 입력값에 맞게 변환하는 transformer를 생성\n",
        "    transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
        "\n",
        "    ## 생성한 transformer로 sentence를 변환하여 저장\n",
        "    self.sentences = [transform([data[sent_idx]]) for data in dataset]\n",
        "    self.labels = [np.int32(data[label_idx]) for data in dataset]\n",
        "  \n",
        "  def __getitem__ (self, i):\n",
        "    return (self.sentences[i] + (self.labels[i], )) # 각 index에 맞는 item 반환 진행 --> 왜 이런 형태인지는 잘 모르겠음\n",
        "  \n",
        "  def __len__(self):\n",
        "    return (len(self.labels))\n",
        "\n",
        "# Parameter setting 진행\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n",
        "\n",
        "# Kobert 모듈에서 제공하는 get_tokenizer와 vocab를 활용해 tokneizer를 구성한다\n",
        "tokenizer = get_tokenizer() \n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(train_set, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(test_set, 0, 1, tok, max_len, True, False)\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert, hidden_size = 768, num_classes=3, dr_rate=None, params=None):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.dr_rate = dr_rate\n",
        "\n",
        "    ## classifier는 선형 회귀 모델로 구성 (input size = 768, output size = 3 (label이 3개로 구성))\n",
        "    self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    ## overfitting 방지를 위한 dropout 비율 설정\n",
        "    if dr_rate:\n",
        "      self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "  # attention mask sequence를 구성해주는 함수 --> padding이 아닌 영역을 0에서 1로 변경\n",
        "  def gen_attention_mask(self, token_ids, valid_length):\n",
        "    attention_mask = torch.zeros_like(token_ids)\n",
        "    for i,v in enumerate(valid_length):\n",
        "      attention_mask[i][:v] = 1\n",
        "    \n",
        "    return attention_mask.float()\n",
        "  \n",
        "  # bert + classifier를 관통하는 forward 연산 진행\n",
        "  def forward(self, token_ids, valid_length, segment_ids):\n",
        "\n",
        "    # attention_mask 계산\n",
        "    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "    # bert에 input 투입, 변수명이 pooler인거 보니 출력 embedding에 mean pooling 적용한 값이지 않을까 추측\n",
        "    _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "\n",
        "    # dropout 비율이 존재한다면, dropout 적용\n",
        "    if self.dr_rate:\n",
        "        out = self.dropout(pooler)\n",
        "\n",
        "    # classifier 진행\n",
        "    return self.classifier(out) \n",
        "\n",
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "model_state_dict = torch.load(\"kobert_classifier.pth\", map_location=device)\n",
        "model.load_state_dict(model_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyfuFn5XR-DW",
        "outputId": "f5068508-8650-4606-9e9a-3b667a45402f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/drive/MyDrive/종합설계/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. predict"
      ],
      "metadata": {
        "id": "pa_XlzITS-bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kiwipiepy import Kiwi\n",
        "kiwi = Kiwi()\n",
        "\n",
        "def predict(predict_sentence):\n",
        "  # 0. 만약 성희롱, 욕설 사전에 걸리지 않으면, 바로 return 0 진행\n",
        "  morphs = kiwi.tokenize(predict_sentence)\n",
        "  flag = False\n",
        "  for morph in morphs:\n",
        "    if morph.form in sexual_list or morph.form in abuse_list:\n",
        "      flag = True\n",
        "      break\n",
        "  \n",
        "  if flag == False:\n",
        "    return 0\n",
        "\n",
        "\n",
        "  # 1. data set 구성 (문장, 라벨)\n",
        "  data = [predict_sentence, '0']\n",
        "  dataset_another = [data]\n",
        "\n",
        "  # 2. data를 bert의 입력에 맞게 변환하기\n",
        "  another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "  test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=0)\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "      token_ids = token_ids.long().to(device)\n",
        "      segment_ids = segment_ids.long().to(device)\n",
        "      valid_length= valid_length\n",
        "      label = label.long().to(device)\n",
        "\n",
        "      # 모델 forward 연산 진행\n",
        "      out = model(token_ids, valid_length, segment_ids)\n",
        "      \n",
        "      # torch out -> numpy 형식으로 변환\n",
        "      logits = out[0].detach().cpu().numpy()\n",
        "\n",
        "      return np.argmax(logits)"
      ],
      "metadata": {
        "id": "5_xgP7MMSEsy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = list()\n",
        "\n",
        "for rows in test_set:\n",
        "  labels = predict(rows[0])\n",
        "  y_hat.append(labels)"
      ],
      "metadata": {
        "id": "6UCRuT6gTDmV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = list()\n",
        "\n",
        "for rows in test_set:\n",
        "  y.append(rows[1])"
      ],
      "metadata": {
        "id": "PMk6AUbaTFSG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "cm = confusion_matrix(y, y_hat)\n",
        "print(\"===Confusion Matrix===\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRreagFFTU1k",
        "outputId": "c9d84c74-d830-4f97-a3e2-88f88b82abda"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===Confusion Matrix===\n",
            "[[1933   39   27]\n",
            " [  62  906   43]\n",
            " [  55   42  763]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Accuracy\n",
        "print(\"\\n===Accuracy===\")\n",
        "print(accuracy_score(y,y_hat))\n",
        "\n",
        "# 3. Precision\n",
        "print(\"\\n===Precision===\")\n",
        "print(precision_score(y ,y_hat, average='macro'))\n",
        "\n",
        "# 4. Recall\n",
        "print(\"\\n===Recall===\")\n",
        "print(recall_score(y,y_hat, average='macro'))\n",
        "\n",
        "\n",
        "# 5. F1\n",
        "print(\"\\n===F1===\")\n",
        "print(f1_score(y, y_hat, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x_ylQRPTXQH",
        "outputId": "5d1e6ea8-3856-47d8-d665-11b905067c79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===Accuracy===\n",
            "0.930749354005168\n",
            "\n",
            "===Precision===\n",
            "0.9256087821740008\n",
            "\n",
            "===Recall===\n",
            "0.9167784091019585\n",
            "\n",
            "===F1===\n",
            "0.921023032425044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8iOy5xZTphK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}