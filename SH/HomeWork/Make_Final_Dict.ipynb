{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/82KJ/ARS-system-for-protecting-telephone-counselor/blob/main/SH/HomeWork/Make_Final_Dict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7QtMgW5chSj"
      },
      "source": [
        "# 1. 준비\n",
        "- cpu 환경은 너무 느려서 코랩에서 부탁드립니다 ㅎ\n",
        "- 꼭 GPU 가속이 켜져있는지 확인해주세요!\n",
        "- 추가로, 구글 드라이브에 학습 데이터, 사전 데이터, 모델을 모두 로드해 주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AXcR1zTUXbp",
        "outputId": "2527e023-6343-4eac-b22d-5a2d743a258a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSBlEIPocGRu",
        "outputId": "804d8372-97e1-4da4-804e-b4b95bb371dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-hsmjt4ce\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-hsmjt4ce\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m426.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonnlp<=0.10.0,>=0.6.0\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
            "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n",
            "  Downloading torch-1.10.1-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (23.3.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.22.4)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.19.0,>=1.18.18\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.34)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.0)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.27.1)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.5.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.65.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.11.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.10.31)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.26,>=1.20\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n",
            "Building wheels for collected packages: kobert, gluonnlp, sacremoses\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15705 sha256=e32d6b6c226bd152b85b7585c286b060f99a98206d5270f39acc256d02c793a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lkeeqn0d/wheels/0b/20/d8/031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp39-cp39-linux_x86_64.whl size=680545 sha256=6b5a106d27194d5453004c90c3e1f5758540752256c028b4a8f3e4701c2c1bc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/17/70/b257bc53879a458c4bfcc900e89271aa8b4f19366a54bd2455\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=b48a8cfae35d5ceea40ef0663eb83abc3ccbbc565c8a7f933a2c9a37107309e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built kobert gluonnlp sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, urllib3, torch, sacremoses, onnxruntime, jmespath, graphviz, gluonnlp, botocore, s3transfer, mxnet, huggingface-hub, transformers, boto3, kobert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "XB4Qc7ArjOdD",
        "outputId": "d12c57c9-a17e-411a-9e33-06f2b4cd68fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kiwipiepy\n",
            "  Downloading kiwipiepy-0.15.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwipiepy-model~=0.15\n",
            "  Downloading kiwipiepy_model-0.15.0.tar.gz (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from kiwipiepy) (1.22.4)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: kiwipiepy-model\n",
            "  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.15.0-py3-none-any.whl size=30602642 sha256=c1a552855bf24e0e23f93a3d607f426876af06842feab69d06ca174e094a4b71\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/34/31/a1682c8249bf2ba89a29498d62c35c08ceb116537b4530d93e\n",
            "Successfully built kiwipiepy-model\n",
            "Installing collected packages: kiwipiepy-model, dataclasses, kiwipiepy\n",
            "Successfully installed dataclasses-0.6 kiwipiepy-0.15.0 kiwipiepy-model-0.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install kiwipiepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl4ydJ25do1d",
        "outputId": "487e4568-1528-4221-a916-ae7b45db1300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjhhqi-DeDXQ"
      },
      "source": [
        "# 2. 데이터 준비\n",
        "- 여기서는 training_dataset.csv를 활용해서 train, test set을 준비하게 됩니다!\n",
        "- 제가 디렉토리에 같이 넣어놀테니까 이거 활용하세요! --> 구글 드라이브로 옮기기\n",
        "- 추가로, 사전도 미리 준비해 놓겠습니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiicrmHBXWnQ",
        "outputId": "2d0fb6ba-8643-4d0f-d7f6-775ca0943dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ARS-system-for-protecting-telephone-counselor/SH/HomeWork\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/ARS-system-for-protecting-telephone-counselor/SH/HomeWork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tO8e36aAd675"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"training_dataset.csv\")\n",
        "\n",
        "data_list = list()\n",
        "for sen, lab in zip(data[\"0\"], data[\"1\"]):\n",
        "  data_list.append([sen,lab])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVIAuKD6efZO",
        "outputId": "37af9609-ce25-4add-80fe-29d475d74caf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['그 동안 보빨러들이 얼마나 잘 해줬겠어ㅋㅋㅋ', 2]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(data_list, test_size=0.1, random_state=0) # train : test = 9:1\n",
        "train_set[0]\n",
        "\n",
        "# 여기서 만들어진 test_set을 중점으로 활용하시면 됩니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEly2gEjehgB",
        "outputId": "f5a2a9bc-ae4a-4613-baf8-5c6d3e173006"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['인정 쟤네 목소리 쫌 심각하게 족같아', 1],\n",
              " ['나 알바하는 곳 너무 장사 잘돼서 힘들어.', 0],\n",
              " ['아슬아슬~ 정말 프로답게 벗어 주네.', 2]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_set[:3]\n",
        "\n",
        "# 아래를 보면 알겠지만, 각 row마다 0번이 문장 데이터, 1번이 라벨링 데이터입니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HW9glruxeoSj"
      },
      "outputs": [],
      "source": [
        "abuse = pd.read_csv(\"폭언사전.csv\")\n",
        "sexual = pd.read_csv(\"성희롱사전.csv\")\n",
        "\n",
        "# 사전은 아래에서 쓰일 거라서 미리 로드합니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "kHMAWMdJfL6e",
        "outputId": "2da1c7f4-21b3-4164-f460-2e56738adcb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0     0\n",
              "0           0    가난\n",
              "1           1  가난뱅이\n",
              "2           2    가두"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bbf4c33d-8c8b-455d-bcd5-111f361c6969\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>가난</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>가난뱅이</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>가두</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbf4c33d-8c8b-455d-bcd5-111f361c6969')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bbf4c33d-8c8b-455d-bcd5-111f361c6969 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bbf4c33d-8c8b-455d-bcd5-111f361c6969');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "abuse.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "yaHt_zcEfOhh",
        "outputId": "143464d1-647a-45cc-e52a-28bb93c5a0c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0     0\n",
              "0           0   19금\n",
              "1           1    69\n",
              "2           2  69자세"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8460f248-49c7-4c4c-97b2-abff385fdbb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>19금</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>69자세</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8460f248-49c7-4c4c-97b2-abff385fdbb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8460f248-49c7-4c4c-97b2-abff385fdbb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8460f248-49c7-4c4c-97b2-abff385fdbb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "sexual.head(3)\n",
        "\n",
        "# 사전이 dataframe으로 로드되면서, 컬럼명[Unnamed:0, 0]으로 자동 지정되었어요!\n",
        "# 여기서 '0' col을 사용하면 되겠죠?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OLibdmZAfhR0"
      },
      "outputs": [],
      "source": [
        "abuse_list = list()\n",
        "sexual_list = list()\n",
        "\n",
        "for rows in abuse[\"0\"]:\n",
        "  abuse_list.append(rows)\n",
        "\n",
        "for rows in sexual[\"0\"]:\n",
        "  sexual_list.append(rows)\n",
        "\n",
        "# dataframe로 load한 사전이 사용하기 불편할 수도 있으니, 리스트 형식으로도 저장합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXMNhsVlf7E-",
        "outputId": "1e5ea3a4-cccd-45e5-fef6-e458d2b98b47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['가난', '가난뱅이', '가두'], ['19금', '69', '69자세'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "abuse_list[:3], sexual_list[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXGKWs2GgBxG"
      },
      "source": [
        "# 3. 코버트 모델 로드\n",
        "- 여기서는 저희가 만든 \"kobert_classifier.pth\"을 로드해서 모델을 준비합니다\n",
        "- 모델은 가지고 계시죠? 그거 구글 드라이브에 로드해서 사용하시면 됩니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtKReUMYf-KY",
        "outputId": "a71a3743-49a5-4c13-be66-684a8576f7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/drive/MyDrive/ARS-system-for-protecting-telephone-counselor/SH/HomeWork/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "\n",
        "    # sentence , label data를 BERT의 입력값에 맞게 변환하는 transformer를 생성\n",
        "    transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
        "\n",
        "    ## 생성한 transformer로 sentence를 변환하여 저장\n",
        "    self.sentences = [transform([data[sent_idx]]) for data in dataset]\n",
        "    self.labels = [np.int32(data[label_idx]) for data in dataset]\n",
        "  \n",
        "  def __getitem__ (self, i):\n",
        "    return (self.sentences[i] + (self.labels[i], )) # 각 index에 맞는 item 반환 진행 --> 왜 이런 형태인지는 잘 모르겠음\n",
        "  \n",
        "  def __len__(self):\n",
        "    return (len(self.labels))\n",
        "\n",
        "# Parameter setting 진행\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n",
        "\n",
        "# Kobert 모듈에서 제공하는 get_tokenizer와 vocab를 활용해 tokneizer를 구성한다\n",
        "tokenizer = get_tokenizer() \n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(train_set, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(test_set, 0, 1, tok, max_len, True, False)\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert, hidden_size = 768, num_classes=3, dr_rate=None, params=None):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.dr_rate = dr_rate\n",
        "\n",
        "    ## classifier는 선형 회귀 모델로 구성 (input size = 768, output size = 3 (label이 3개로 구성))\n",
        "    self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    ## overfitting 방지를 위한 dropout 비율 설정\n",
        "    if dr_rate:\n",
        "      self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "  # attention mask sequence를 구성해주는 함수 --> padding이 아닌 영역을 0에서 1로 변경\n",
        "  def gen_attention_mask(self, token_ids, valid_length):\n",
        "    attention_mask = torch.zeros_like(token_ids)\n",
        "    for i,v in enumerate(valid_length):\n",
        "      attention_mask[i][:v] = 1\n",
        "    \n",
        "    return attention_mask.float()\n",
        "  \n",
        "  # bert + classifier를 관통하는 forward 연산 진행\n",
        "  def forward(self, token_ids, valid_length, segment_ids):\n",
        "\n",
        "    # attention_mask 계산\n",
        "    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "    # bert에 input 투입, 변수명이 pooler인거 보니 출력 embedding에 mean pooling 적용한 값이지 않을까 추측\n",
        "    _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "\n",
        "    # dropout 비율이 존재한다면, dropout 적용\n",
        "    if self.dr_rate:\n",
        "        out = self.dropout(pooler)\n",
        "\n",
        "    # classifier 진행\n",
        "    return self.classifier(out) \n",
        "\n",
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "model_state_dict = torch.load(\"kobert_classifier.pth\", map_location=device)\n",
        "model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDFnsU6tgty1"
      },
      "source": [
        "# 4. test_set 예측 라벨 산출\n",
        "- 이제 3870개의 test 문장을 모델에 투입해서, 예측된 라벨링을 얻을 거예요!\n",
        "- 결과 라벨은 y_hat에 저장할게요~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mfT_dl6Cgm2s"
      },
      "outputs": [],
      "source": [
        "def predict(predict_sentence):\n",
        "  # 1. data set 구성 (문장, 라벨)\n",
        "  data = [predict_sentence, '0']\n",
        "  dataset_another = [data]\n",
        "\n",
        "  # 2. data를 bert의 입력에 맞게 변환하기\n",
        "  another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "  test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=0)\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "      token_ids = token_ids.long().to(device)\n",
        "      segment_ids = segment_ids.long().to(device)\n",
        "      valid_length= valid_length\n",
        "      label = label.long().to(device)\n",
        "\n",
        "      # 모델 forward 연산 진행\n",
        "      out = model(token_ids, valid_length, segment_ids)\n",
        "      \n",
        "      # torch out -> numpy 형식으로 변환\n",
        "      logits = out[0].detach().cpu().numpy()\n",
        "\n",
        "      return np.argmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fD4fCOmuhGjL"
      },
      "outputs": [],
      "source": [
        "y_hat = list()\n",
        "\n",
        "for rows in test_set:\n",
        "  labels = predict(rows[0])\n",
        "  y_hat.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKNKsW_OhPLm",
        "outputId": "5758ce2f-4741-4327-8cba-60c1313229c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "y_hat[:3]\n",
        "\n",
        "# 이게 저희 모델로 예측한 라벨 값들 입니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6kLOD90dzvr",
        "outputId": "f2b9386b-7348-4d54-b486-da7025db7aa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GZZfB9Szhg7V"
      },
      "outputs": [],
      "source": [
        "y = list()\n",
        "\n",
        "for rows in test_set:\n",
        "  y.append(rows[1])\n",
        "\n",
        "# test_set은 [문장, 실제 라벨값]의 구조라서 실제 라벨값만 따로 y에 저장했어요 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-fwsxlNhkpa",
        "outputId": "e73135a7-75c8-4f9f-a83d-38378dfed579"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "y[:3]\n",
        "\n",
        "# 요건 실제 라벨값 입니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRfW_UXnd5oj",
        "outputId": "5507a0c3-1e94-442e-9dd4-e3bc5c79d132"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq-6AuNwiM7i",
        "outputId": "8efeb559-74d4-47dd-f82b-9dbb51935010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===Confusion Matrix===\n",
            "[[1912   54   33]\n",
            " [  62  906   43]\n",
            " [  55   42  763]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "cm = confusion_matrix(y, y_hat)\n",
        "print(\"===Confusion Matrix===\")\n",
        "print(cm)\n",
        "\n",
        "# y, y_hat으로 confusion matrix을 산출한 결과입니다\n",
        "# row는 실제 라벨\n",
        "# col은 예측 라벨\n",
        "# 즉, 0행은 실제 일반 문장 1999개에서 (1912 + 54 + 33) 1912개는 올바르게 평가, 54개는 폭언으로 잘못 산출, 33개는 성희롱으로 잘못 산출 했다는 것을 의미합니다\n",
        "# 우리의 목표는 54와 33으로 잘못 판별된 문장들을 모델 투입 전에 사전에서 미리 거를 수 있도록, 사전, 문장 각색을 진행하는 것입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrfjAfsmgsHx"
      },
      "source": [
        "y_hat과 y 비교하여 다른 값 index_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyRnHJaEd-JQ",
        "outputId": "a7da26be-649c-44c9-f9c2-8db0d5e36b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 21, 24, 25, 33, 34, 93, 101, 104, 117, 132, 135, 139, 142, 150, 155, 159, 162, 171, 205, 213, 225, 246, 255, 270, 285, 300, 301, 309, 327, 336, 368, 387, 388, 400, 413, 414, 417, 423, 430, 466, 470, 492, 511, 518, 527, 569, 577, 591, 621, 622, 629, 651, 658, 672, 676, 677, 678, 681, 714, 715, 716, 747, 761, 774, 788, 798, 802, 807, 817, 824, 832, 835, 875, 882, 895, 907, 942, 945, 954, 961, 963, 964, 974, 990, 1002, 1010, 1047, 1058, 1067, 1071, 1079, 1091, 1099, 1102, 1105, 1108, 1149, 1168, 1204, 1219, 1233, 1238, 1267, 1310, 1315, 1332, 1371, 1372, 1389, 1390, 1391, 1396, 1436, 1441, 1456, 1457, 1472, 1501, 1510, 1521, 1528, 1531, 1537, 1563, 1593, 1632, 1638, 1642, 1674, 1696, 1697, 1704, 1736, 1745, 1750, 1761, 1773, 1777, 1786, 1810, 1815, 1819, 1832, 1840, 1872, 1884, 1909, 1935, 1940, 1974, 1996, 2034, 2039, 2040, 2059, 2078, 2079, 2092, 2097, 2105, 2114, 2115, 2143, 2197, 2203, 2208, 2217, 2225, 2245, 2249, 2267, 2297, 2310, 2312, 2353, 2368, 2371, 2378, 2392, 2394, 2397, 2426, 2430, 2441, 2451, 2462, 2469, 2471, 2474, 2499, 2523, 2560, 2561, 2564, 2573, 2602, 2610, 2619, 2627, 2657, 2677, 2682, 2692, 2698, 2700, 2704, 2706, 2707, 2740, 2742, 2745, 2752, 2758, 2760, 2765, 2807, 2811, 2884, 2909, 2914, 2918, 2939, 2942, 2960, 2987, 2998, 3019, 3026, 3027, 3030, 3033, 3044, 3051, 3074, 3080, 3097, 3118, 3130, 3143, 3167, 3177, 3190, 3191, 3238, 3249, 3273, 3290, 3298, 3341, 3342, 3343, 3347, 3350, 3375, 3376, 3388, 3393, 3397, 3398, 3407, 3409, 3459, 3477, 3532, 3534, 3549, 3554, 3587, 3600, 3626, 3654, 3685, 3690, 3711, 3720, 3726, 3731, 3744, 3746, 3747, 3749, 3753, 3766, 3767, 3789, 3796, 3849, 3854]\n"
          ]
        }
      ],
      "source": [
        "index_list = []\n",
        "\n",
        "for i in range (len(y)):\n",
        "  if y[i] != y_hat[i]:\n",
        "    index_list.append(i)\n",
        "\n",
        "print(index_list) #289\n",
        "#print(len(index_list)) \n",
        "#근데 사실 이건 일반 아닌데 다르게 예측한 것도 포함되니깐 밑에 껄로 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PFSS5s2jZPF"
      },
      "source": [
        "# 5. 사전 각색 or 문장 각색\n",
        "- 이제 본격적인 알고리즘 작성이 필요합니다\n",
        "- 대략적인 순서는 제가 여기에 적어둘게여\n",
        "  - y 와 y_hat을 비교하면서, y==0 인데, y_hat != 0이 아닌 문장의 인덱스를 구합니다!\n",
        "  - 이제, 구한 인덱스를 바탕으로 test_set에서 문장을 찾아서, 형태소 분해 후 저장을 합니다\n",
        "  - 저장된 형태소 리스트를 바탕으로, 각각 사전의 어떤 키워드와 매칭되는지 확인합니다!\n",
        "  - 마지막으로, 사전 각색 혹은 문장 각색 두 방법 중 하나를 선택해서 진행하시면 됩니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0Js6NrziU4B",
        "outputId": "9ea41a08-e32d-4840-8384-c3dcfc57e473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# 1. y 와 y_hat을 비교하면서, y==0 인데, y_hat != 0이 아닌 문장의 인덱스를 구합니다! (== 실제 일반 문장인데, 폭언, 성희롱으로 잘못 예측된 문장의 인덱스)\n",
        "\n",
        "moral_abuse_idx = list() # 일반인데 폭언으로 잘못 판별된 문장의 인덱스 리스트\n",
        "moral_sexual_idx = list() # 일반인데 성희롱으로 잘못 판별된 문장의 인덱스 리스트\n",
        "\n",
        "for idx in range(len(y)):\n",
        "  if y[idx] == 0 and y_hat[idx] == 1:\n",
        "    moral_abuse_idx.append(idx)\n",
        "  elif y[idx] == 0 and y_hat[idx] == 2:\n",
        "    moral_sexual_idx.append(idx)\n",
        "\n",
        "\n",
        "# 구한 인덱스의 길이가 confusion matrix의 54, 33으로 나오겠죠?\n",
        "len(moral_abuse_idx), len(moral_sexual_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eifCjgOaoC2N",
        "outputId": "ad46cd30-649d-4f81-9231-407565ca5360"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[33,\n",
              " 104,\n",
              " 135,\n",
              " 171,\n",
              " 246,\n",
              " 368,\n",
              " 413,\n",
              " 417,\n",
              " 577,\n",
              " 672,\n",
              " 677,\n",
              " 716,\n",
              " 798,\n",
              " 961,\n",
              " 974,\n",
              " 1047,\n",
              " 1204,\n",
              " 1310,\n",
              " 1332,\n",
              " 1372,\n",
              " 1396,\n",
              " 1521,\n",
              " 1528,\n",
              " 1537,\n",
              " 1697,\n",
              " 1810,\n",
              " 2040,\n",
              " 2143,\n",
              " 2203,\n",
              " 2368,\n",
              " 2451,\n",
              " 2469,\n",
              " 2471,\n",
              " 2474,\n",
              " 2564,\n",
              " 2657,\n",
              " 2742,\n",
              " 2752,\n",
              " 2909,\n",
              " 2914,\n",
              " 3027,\n",
              " 3097,\n",
              " 3167,\n",
              " 3238,\n",
              " 3343,\n",
              " 3393,\n",
              " 3477,\n",
              " 3600,\n",
              " 3626,\n",
              " 3744,\n",
              " 3746,\n",
              " 3747,\n",
              " 3789,\n",
              " 3849]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "moral_abuse_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q99cEojgoJcN",
        "outputId": "f8c4dd56-6c35-44ff-812b-3eb5943b6779"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25,\n",
              " 142,\n",
              " 155,\n",
              " 159,\n",
              " 285,\n",
              " 301,\n",
              " 569,\n",
              " 1091,\n",
              " 1102,\n",
              " 1168,\n",
              " 1219,\n",
              " 1389,\n",
              " 1436,\n",
              " 1642,\n",
              " 1750,\n",
              " 1761,\n",
              " 2078,\n",
              " 2097,\n",
              " 2115,\n",
              " 2267,\n",
              " 2758,\n",
              " 2765,\n",
              " 2807,\n",
              " 3074,\n",
              " 3177,\n",
              " 3341,\n",
              " 3347,\n",
              " 3375,\n",
              " 3388,\n",
              " 3532,\n",
              " 3534,\n",
              " 3749,\n",
              " 3854]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "moral_sexual_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUHO8HX2kwEE",
        "outputId": "ba4f865a-29ec-47a8-d327-60b9a6fc4b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33, ['야', '짱개', '언어', '그냥', '모르', '어도', '되', '거든', '?']]\n",
            "[25, ['성관계', '끝나', '었', '는데', '도', '계속', '이쁘', '다고', '하', '어', '주', '고', '이름', '이랑', '전화', '번호', '도', '알리', '어', '주', '네', '.']]\n"
          ]
        }
      ],
      "source": [
        "# 2. 찾은 인덱스를 바탕으로 test_set에서 문장을 찾고, 형태소 분해 후 저장해요\n",
        "from kiwipiepy import Kiwi\n",
        "kiwi = Kiwi()\n",
        "\n",
        "moral_abuse_morphs = list()    # 일반인데 폭언으로 잘못 판별된 문장의 형태소 리스트\n",
        "moral_sexual_morphs = list()   # 일반인데 성희롱으로 잘못 판별된 문장의 형태소 리스트\n",
        "\n",
        "#(1) 폭언\n",
        "for idx in moral_abuse_idx: #폭언으로 잘못판별된 문장의 리스트 인덱스로\n",
        "  morphs = kiwi.tokenize(test_set[idx][0]) #테스트 셋 문장을 찾고 형태소 분해\n",
        "  temp = list()\n",
        "  for morph in morphs:\n",
        "    temp.append(morph.form)\n",
        "  moral_abuse_morphs.append([idx,temp])\n",
        "\n",
        "#(2) 성희롱\n",
        "for idx in moral_sexual_idx:\n",
        "  morphs = kiwi.tokenize(test_set[idx][0])\n",
        "  temp = list()\n",
        "  for morph in morphs:\n",
        "    temp.append(morph.form)\n",
        "  moral_sexual_morphs.append([idx,temp])\n",
        "\n",
        "print(moral_abuse_morphs[0]) #이거는 잘못 판별된 애들\n",
        "print(moral_sexual_morphs[0])\n",
        "\n",
        "# 출력된 결과를 보면, 짱개, 성관계 등의 형태소가 포함되어 있어서 두 문장에 대한 표현은 사실 일반 문장이라고 보기 힘듭니다.. ㅜ\n",
        "# 아래 문장들은 사실, 일반 문장 데이터 만들 때 제거되었어야 맞는데.. 흠.. 누군가 대충 넘어가서 이런 결과가.. ㅎ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynliQ3wHmXfS",
        "outputId": "efe3f03a-4373-4460-9bdb-79517f263997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33, ['야', '짱개', '언어', '그냥', '모르', '어도', '되', '거든', '?'], ['짱개', '어도'], ['어도']]\n",
            "\n",
            "[104, ['걔', '네', '이름', '부터', '정', '떨어지', 'ᆷ', 'ㅋㅋ', '평생', '땅', '치', '고', '후회', '하', '었', '으면'], ['정', '떨어지', '땅', '치'], ['정', '으면']]\n",
            "\n",
            "[135, ['안락사', '는', '죽음', '을', '존엄', '하', '게', '맞', '을', '수', '있', '게', '돕', '자는', '거', '이', 'ᆫ데', '.'], ['안락사', '죽음', '맞', '자는'], ['자는']]\n",
            "\n",
            "[171, ['저번', '대선', '홍', '이', '문', '개털', '었', '는데', '?', '탄핵', '때문', '에', '지', 'ᆫ', '거', '이', '야'], ['홍'], ['는데']]\n",
            "\n",
            "[246, ['너', '아이스크림', '먹', '고', '싶', '다며', '나', '가', '배', '민', '으로', '시키', '어', '주', 'ᆯ까', '?'], ['먹', '시키'], ['먹', '배', '시키', '주']]\n",
            "\n",
            "[368, ['나', '그냥', '멍', '때리', '고', '있', '었', '어'], ['멍', '때리'], ['때리']]\n",
            "\n",
            "[413, ['감방', '가', '어서', '도', '또', '사기치', '어', '먹', '을', '궁리', '하', '겠', '지', '점점', '더', '지능', '것', '으로', '크', 'ᆫ', '사기치력', '이', '고', '열심히', '연구', '하', '겠', '구만', '하', '는', '넘', '은', '또', '하', '지', '.'], ['감방', '사기치', '먹', '지능', '크', '넘'], ['어서', '먹', '크']]\n",
            "\n",
            "[417, ['아니야', '장애', '있', '는', '사람', '들', '도', '결혼', '잘', '하', '어서', '아이', '도', '잘', '낳', '을', '수', '있', '게', '하', '어', '주', '어야지'], ['장애', '사람', '들', '잘', '잘', '낳', '어야지'], ['장애', '들', '잘', '어서', '잘', '낳', '주']]\n",
            "\n",
            "[577, ['형국', '을', '알', '어야', '너', '같', '은', '놈', '한테', '도', '짓밟히', '지', '않', '을', '거', '알', '냐'], ['놈'], ['알', '어야', '한테', '알', '냐']]\n",
            "\n",
            "[672, ['저', '놈', '아버지', '가', '일본', '놈', '밑', '에서', '마름질', '하', '었었', '어', '.'], ['놈', '놈', '밑'], ['밑']]\n",
            "\n",
            "[677, ['아', '진짜', '?', '슬슬', '단물', '빠지', 'ᆫ', '듯', '때리', '어', '치', '어라'], ['빠지', '때리', '치', '어라'], ['진짜', '빠지', '때리', '어라']]\n",
            "\n",
            "[716, ['판사', '가족', '이', '당하', '었', '으면', '사형', '감', '이', '지', '.'], ['가족', '당하', '사형', '감'], ['당하', '으면', '감']]\n",
            "\n",
            "[798, ['지지자', '들', '은', '지', 'ᆯ', '때', '도', '조용히', '지', '게', '내버리', '어', '두', '지', 'ᆯ', '않', '네', '.'], ['들', '내버리', '두'], ['들', '때']]\n",
            "\n",
            "[961, ['삼시샛기', '차리', '어', '주', '니까', '고맙', '은', '줄', '도', '모르', '고', '.', '이제', '부터', '너', '가', '하', '어', '보', 'ᆯ래', '?'], ['차리', '줄'], ['주', '니까', '줄', '보']]\n",
            "\n",
            "[974, ['그냥', '좀', '닥치', '고', '게임', '이나', '하', '지', '?'], ['닥치', '이나'], ['좀', '이나']]\n",
            "\n",
            "[1047, ['나', 'ᆫ', '시체', '토막', '을', '보', '면', '잔뜩', '흥분', '되', '더라'], ['시체', '토막'], ['보', '흥분', '더라']]\n",
            "\n",
            "[1204, ['남자', '들', '은', '다', '도살', '시키', '어', '버리', '었', '음', '좋', '겠', '어', '세계', '평화', '는', '남자', '사살', '이', '지', '뭐'], ['들', '도살', '시키', '버리', '좋', '사살'], ['남자', '들', '시키', '버리', '좋', '남자']]\n",
            "\n",
            "[1310, ['1', '단계', '는', '솔직히', '부모', '입김', '아니', '었', '으면', '발끝', '도', '못', '따라가', '었', '다'], ['부모'], ['으면']]\n",
            "\n",
            "[1332, ['나', '는', '일찍', '죽', '을', '만', '하', 'ᆫ', '멍청하', 'ᆫ', '짓', '은', '하', '지', '않', '을래'], ['죽', '멍청하', '짓'], ['죽', '짓']]\n",
            "\n",
            "[1372, ['아', '진짜', '개빡', '치', 'ᆫ다'], ['개빡', '치'], ['진짜']]\n",
            "\n",
            "[1396, ['님', '존예보스', '이', 'ᆫ', '거', '같', '어요', '.'], [], []]\n",
            "\n",
            "[1521, ['걔', '는', '그', '사람', '잘생기', '었', '다고', '지지', '하', '는', '빠돌', '이', '더라'], ['사람', '지지'], ['잘생기', '더라']]\n",
            "\n",
            "[1528, ['뭐', '타투', '하', 'ᆯ', '수', '는', '있', '는데', '저렇', '게', '팔', '덮', '은', '자식', '이랑', '은', '겸상', '도', '하', '지', '말', '어'], ['팔', '자식', '말'], ['는데', '팔', '말']]\n",
            "\n",
            "[1537, ['저런', '미개', '하', 'ᆫ', '것', '들', '을', '우리', '가', '왜', '따라하', 'ᆫ다고', '헛', '소리', '하', '는', '거', '이', '야', '?'], ['미개', '들', '헛', '소리'], ['미개', '들', '우리']]\n",
            "\n",
            "[1697, ['나', '어제', '하체', '헬스', '로', '조지', '었', '더니', '못', '걷', '겠', '어'], ['조지', '걷'], ['하체']]\n",
            "\n",
            "[1810, ['저러', '다', '떨어지', '면', '어쩌', '려고', '그렇', '어', '?', '뒤지', '려면', '지', '혼자', '뒤지', '지'], ['떨어지', '뒤지', '뒤지'], ['저러', '혼자']]\n",
            "\n",
            "[2040, ['이혼', '소송', '지', '어서', '돈', '이나', '줄창', '뜯기', '었', '으면', '좋', '겠', '다', '.'], ['이나', '뜯기', '좋'], ['어서', '돈', '이나', '으면', '좋']]\n",
            "\n",
            "[2143, ['그니까', '걍', '쓰레기', '같', '은', '애', '들', '이', '다'], ['쓰레기', '들'], ['들']]\n",
            "\n",
            "[2203, ['참교육', '가', '어야', '겠', '네'], ['참교육'], ['어야']]\n",
            "\n",
            "[2368, ['아니', '근데', '지', '얘기', '도', '재밌', '으면', '모르', '어', '재미', '도', '디지게', '없', '어', '가지', '고', 'ㅋㅋ'], ['디지게', '가지'], ['으면', '가지']]\n",
            "\n",
            "[2451, ['아', '배', '아프', '어', '요새', '변비', '이', 'ᆫ지', '똥', '이', '안', '나오', '어', 'ㅠㅠ', '미치', '겠', '어'], ['똥', '나오', '미치'], ['배', '변비', '똥']]\n",
            "\n",
            "[2469, ['가만히', '있', '어', '이', '벌레', '이', '야', '.'], ['벌레'], []]\n",
            "\n",
            "[2471, ['방역', '못', '하', '면', '돌멩이', '던지', 'ᆯ', '거', '이', '다'], ['돌멩이', '던지'], ['던지']]\n",
            "\n",
            "[2474, ['진짜', '철딱서니', '가', '없', '어도', '너무', '없', '어'], ['어도'], ['진짜', '어도']]\n",
            "\n",
            "[2564, ['부모', '세대', '에', '는', '자식', '없', '으면', '그냥', '장애인', '취급', '당하', '던', '사회', '이', '었', '는데', '가난', '하', 'ᆫ', '부모', '들', '이', '좋', '어서', '애', '를', '낳', '었', '겠', '냐'], ['부모', '자식', '장애인', '취급', '당하', '사회', '가난', '부모', '들', '좋', '낳'], ['으면', '당하', '는데', '들', '좋', '어서', '낳', '냐']]\n",
            "\n",
            "[2657, ['진짜', '쓰', 'ᆯ', '때', '마다', '개', '빡', '치', '네'], ['쓰', '개', '빡', '치'], ['진짜', '쓰', '때', '마다', '빡']]\n",
            "\n",
            "[2742, ['도련님', '못', '하', '겠', '으면', '아이', '삼촌', '이', '라고', '하', '어라'], ['어라'], ['으면', '어라']]\n",
            "\n",
            "[2752, ['그냥', '개취', '같', '더라고'], [], []]\n",
            "\n",
            "[2909, ['야', '걔', '네', '도', '나름', '개과천선', '하', '어서', '열심히', '하', 'ᆫ', '거', '이', '야'], [], ['어서']]\n",
            "\n",
            "[2914, ['너', '진짜', '핑프', '같', '어'], [], ['진짜']]\n",
            "\n",
            "[3027, ['나', '는', '학교', '가', 'ᆯ', '준비', '도', '하', '어야', '하', '고', '보통', '은', '엄마', '가', '차리', '어', '주', '지', '않', '어', '너', '는', '니', '가', '차리', '어', '먹', '냐', '??'], ['엄마', '차리', '차리', '먹'], ['어야', '엄마', '주', '먹', '냐']]\n",
            "\n",
            "[3097, ['여보', '식', '세기', '한', '대', '사', '자', '설거지', '하', '기', '힘들', '어', 'ㅜㅜ'], ['식', '대'], ['대', '사', '자', '설거지']]\n",
            "\n",
            "[3167, ['덜미', '만', '잡히', 'ᆫ', '것', '이', '아니', '라', '모가지', '까지', '잡히', 'ᆫ', '거', '이', 'ᆸ니다', '.'], ['모가지'], ['까지']]\n",
            "\n",
            "[3238, ['너', '지금', '부리', '또', '사', '어', '오', 'ᆫ', '거', '이', '고', '기', '말', '고', '뭐', '가', '들', '어', '있', '는', '거', '이', '야', '?'], ['부리', '말', '들'], ['사', '말', '들']]\n",
            "\n",
            "[3343, ['아무리', '국선', '변호사', '이', '라도', '양심', '이', '있', '으면', '죄', '를', '인정', '하', 'ᆫ다고', '하', '어야지', ',', '저런', '애', '들', '이', '사회', '에', '나오', '어서', '개', '차', '반', '으로', '살', 'ᆯ', '거', '뻔히', '알', '면서', '.'], ['라도', '양심', '죄', '인정', '어야지', '들', '사회', '나오', '개', '반', '살', '면서'], ['으면', '들', '어서', '살', '알']]\n",
            "\n",
            "[3393, ['맞', '어', '.', '나', '는', '살인', '이랑', '동급', '이', '라고', '보', '어'], ['맞', '살인'], ['보']]\n",
            "\n",
            "[3477, ['사형', '은', '시키', '면', '안', '되', '어'], ['사형', '시키'], ['시키']]\n",
            "\n",
            "[3600, ['1000', '원', '에', '반찬', '세', '개', '랑', '밥', ',', '국', '나오', '면', '혜자', '아니', 'ᆷ', '?'], ['개', '밥', '국', '나오'], ['세', '밥']]\n",
            "\n",
            "[3626, ['나', '도', '흑인', '으로', '살', 'ᆯ', '바', '에', 'ᆫ', '그냥', '다음', '생', '도박', '하', 'ᆯ', '듯'], ['흑인', '살', '도박'], ['흑인', '살']]\n",
            "\n",
            "[3744, ['남', '의', '나라', '에서', '돈', '받', '어', '가', '는', '주제', '에', '감', '내놔라', '배', '내', '어', '놓', '어라', '하', '네', '.'], ['주제', '감', '놓', '어라'], ['돈', '받', '감', '배', '놓', '어라']]\n",
            "\n",
            "[3746, ['장사', '하', '는', '사람', '들', '은', '물건', '안', '팔리', '면', '가게', '다', '불', '지르', '어야', '하', '나', '?'], ['사람', '들', '불', '지르'], ['장사', '들', '물건', '불', '지르', '어야']]\n",
            "\n",
            "[3747, ['성형', '괴물', '길거리', '에', '좀', '안', '나오', '었', '으면', '좋', '겠', '다'], ['괴물', '길거리', '나오', '좋'], ['좀', '으면', '좋']]\n",
            "\n",
            "[3789, ['저', '예쁘', 'ᆫ', '아가', '를', '어떻', '게', '굶기', '어', '죽이', '냐'], ['예쁘', '아가', '굶기', '죽이'], ['예쁘', '어떻', '죽이', '냐']]\n",
            "\n",
            "[3849, ['나', '는', '말', '하', '는', '감자', '이', '야', '.'], ['말', '감자'], ['말']]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. 우선, 폭언으로 잘못 판별된 문장들이 사전의 어느 부분이랑 매칭되는지 확인해볼까요?\n",
        "# 이때, 사전은 폭언, 성희롱 사전 모두 사용하셔야 됩니다 --> 실제 시스템에서 두개다 매칭을 진행할거라서 ㅇㅇ\n",
        "\n",
        "\n",
        "#잘못 판별된 애들이 사전에 어디랑 매칭되어 있는지 확인 \n",
        "for rows in moral_abuse_morphs:\n",
        "  abuse_matched = list()\n",
        "  sexual_matched = list()\n",
        "\n",
        "  for morph in rows[1]:\n",
        "    if morph in abuse_list:\n",
        "      abuse_matched.append(morph)\n",
        "    if morph in sexual_list:\n",
        "      sexual_matched.append(morph)\n",
        "\n",
        "  rows.append(abuse_matched)\n",
        "  rows.append(sexual_matched)\n",
        "\n",
        "for rows in moral_abuse_morphs:\n",
        "  print(rows)\n",
        "  print()\n",
        "\n",
        "# 폭언으로 잘못 분류된 문장 모두에 대해 (test_set 인덱스, 문장 형태소, 폭언 사전 매칭 형태소, 성희롱 사전 매칭 형태소)\n",
        "# 보면 1396이랑,2752 인덱스는 어떤 사전이랑도 매칭되는 게 없잖아요. 바로 해당 문장들이 사전을 적용하게 되면, 매칭이 되는게 없어서 바로 일반 문장으로 판단되는 문장들입니다!\n",
        "# 제가 말했었죠. 그 사전 적용하면 딱 2문장만 걸러진다고요. 그게 바로 이 두문장입니다!!\n",
        "# 그러니까, 지금 저희가 하는 일이. 이 두문장 말고도 사전에 매칭되는 문장이 없게끔 문장, 사전을 각색하는 일입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5QRxqCQKG_M"
      },
      "source": [
        "여기서부터 사전 각색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8dHmjgxdopUM"
      },
      "outputs": [],
      "source": [
        "# 4. 사전 각색 방법\n",
        "\n",
        "# 위에 문장 중에서, 171 인덱스 문장을 바탕으로 사전을 각색하는게 좋아보여요\n",
        "# 폭언 사전에서는 '홍', 성희롱 사전에서는 '는데'만 매칭되었거든요\n",
        "# 그럼 해당 키워드를 제거하기 전에, 기존에 '홍', '는데' 키워드만 가지고 있던 폭언, 성희롱 문장이 있을 수도 있어요\n",
        "# 그런 문장이 있는지 확인해봐여\n",
        "\n",
        "def isOnlyThatKeyword(keyword):\n",
        "  res = list()\n",
        "\n",
        "  for rows in test_set:\n",
        "    if rows[1] == 1 or rows[1] == 2:  \n",
        "      temp_all = list()\n",
        "      temp_abuse = list()\n",
        "      temp_sexual = list()\n",
        "      morphs = kiwi.tokenize(rows[0])\n",
        "\n",
        "      flag = False\n",
        "      for morph in morphs:\n",
        "        if morph.form == keyword:\n",
        "          flag =True\n",
        "      \n",
        "      if flag == True:\n",
        "        for morph in morphs:\n",
        "          temp_all.append(morph.form)\n",
        "\n",
        "          if morph.form in abuse_list:\n",
        "            temp_abuse.append(morph.form)\n",
        "          if morph.form in sexual_list:\n",
        "            temp_sexual.append(morph.form)\n",
        "        \n",
        "      if len(temp_abuse) + len(temp_sexual) == 1:\n",
        "        res.append([temp_all, temp_abuse, temp_sexual])\n",
        "      \n",
        "\n",
        "  return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWPguDAru045",
        "outputId": "36231aa9-850f-466d-be6f-78e4f533c356"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "isOnlyThatKeyword('홍')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDSuoOgowPW_",
        "outputId": "b134a1ab-5ce4-489b-e638-b3a0c3db676d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "isOnlyThatKeyword('는데') # 홍, 는데만으로 매칭되는 폭언, 성희롱 문장은 업네여!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPnr3zJQNIh"
      },
      "source": [
        "한번에 isOnlyThatKeyword하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc9mLKUuTbpX"
      },
      "outputs": [],
      "source": [
        "temp_abuse_list = [] #매칭된 것 중에 폭언 매칭 한번에 담은 리스트\n",
        "temp_sexual_list = [] #매칭된 것 중에 성희롱 매칭 한번에 담은 리스트\n",
        "for rows in moral_abuse_morphs:\n",
        "  for item in rows[2]:\n",
        "    temp_abuse_list.append(item)\n",
        "  \n",
        "  for item in rows[3]:\n",
        "    temp_sexual_list.append(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIkEpzIWpHMU",
        "outputId": "6288e5f3-b355-4a3d-d08e-33d29cb2ef7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "idx_list = []\n",
        "for rows in moral_abuse_morphs:\n",
        "  for item in str(rows[0]):\n",
        "    idx_list.append(item)\n",
        "\n",
        "print(len(idx_list))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEneRFSfq2uN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91pqSkIMV2cO",
        "outputId": "4dbe8905-c675-40da-983e-497ed08465fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['어도',\n",
              " '정',\n",
              " '으면',\n",
              " '자는',\n",
              " '는데',\n",
              " '먹',\n",
              " '배',\n",
              " '시키',\n",
              " '주',\n",
              " '때리',\n",
              " '어서',\n",
              " '먹',\n",
              " '크',\n",
              " '장애',\n",
              " '들',\n",
              " '잘',\n",
              " '어서',\n",
              " '잘',\n",
              " '낳',\n",
              " '주',\n",
              " '알',\n",
              " '어야',\n",
              " '한테',\n",
              " '알',\n",
              " '냐',\n",
              " '밑',\n",
              " '진짜',\n",
              " '빠지',\n",
              " '때리',\n",
              " '어라',\n",
              " '당하',\n",
              " '으면',\n",
              " '감',\n",
              " '들',\n",
              " '때',\n",
              " '주',\n",
              " '니까',\n",
              " '줄',\n",
              " '보',\n",
              " '좀',\n",
              " '이나',\n",
              " '보',\n",
              " '흥분',\n",
              " '더라',\n",
              " '남자',\n",
              " '들',\n",
              " '시키',\n",
              " '버리',\n",
              " '좋',\n",
              " '남자',\n",
              " '으면',\n",
              " '죽',\n",
              " '짓',\n",
              " '진짜',\n",
              " '잘생기',\n",
              " '더라',\n",
              " '는데',\n",
              " '팔',\n",
              " '말',\n",
              " '미개',\n",
              " '들',\n",
              " '우리',\n",
              " '하체',\n",
              " '저러',\n",
              " '혼자',\n",
              " '어서',\n",
              " '돈',\n",
              " '이나',\n",
              " '으면',\n",
              " '좋',\n",
              " '들',\n",
              " '어야',\n",
              " '으면',\n",
              " '가지',\n",
              " '배',\n",
              " '변비',\n",
              " '똥',\n",
              " '던지',\n",
              " '진짜',\n",
              " '어도',\n",
              " '으면',\n",
              " '당하',\n",
              " '는데',\n",
              " '들',\n",
              " '좋',\n",
              " '어서',\n",
              " '낳',\n",
              " '냐',\n",
              " '진짜',\n",
              " '쓰',\n",
              " '때',\n",
              " '마다',\n",
              " '빡',\n",
              " '으면',\n",
              " '어라',\n",
              " '어서',\n",
              " '진짜',\n",
              " '어야',\n",
              " '엄마',\n",
              " '주',\n",
              " '먹',\n",
              " '냐',\n",
              " '대',\n",
              " '사',\n",
              " '자',\n",
              " '설거지',\n",
              " '까지',\n",
              " '사',\n",
              " '말',\n",
              " '들',\n",
              " '으면',\n",
              " '들',\n",
              " '어서',\n",
              " '살',\n",
              " '알',\n",
              " '보',\n",
              " '시키',\n",
              " '세',\n",
              " '밥',\n",
              " '흑인',\n",
              " '살',\n",
              " '돈',\n",
              " '받',\n",
              " '감',\n",
              " '배',\n",
              " '놓',\n",
              " '어라',\n",
              " '장사',\n",
              " '들',\n",
              " '물건',\n",
              " '불',\n",
              " '지르',\n",
              " '어야',\n",
              " '좀',\n",
              " '으면',\n",
              " '좋',\n",
              " '예쁘',\n",
              " '어떻',\n",
              " '죽이',\n",
              " '냐',\n",
              " '말']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "temp_sexual_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "gjG-NLs8mBz2",
        "outputId": "d43790d3-e1aa-4016-e391-53eec76ae6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-cc781574c6e5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_abuse_list\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misOnlyThatKeyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#왜 안나올까?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-642ea477d599>\u001b[0m in \u001b[0;36misOnlyThatKeyword\u001b[0;34m(keyword)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mtemp_abuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mtemp_sexual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mmorphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkiwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/kiwipiepy/_wrap.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, match_options, normalize_coda, z_coda, split_complex, split_sents, stopwords, echo, blocklist)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         '''\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_coda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_coda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_complex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocklist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocklist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m     def split_into_sents(self, \n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/kiwipiepy/_wrap.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text, match_options, normalize_coda, z_coda, split_complex, split_sents, stopwords, echo, blocklist)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mecho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_refine_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatch_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocklist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocklist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_refine_result_with_echo\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mecho\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_refine_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatch_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mecho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocklist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocklist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for item in temp_abuse_list :\n",
        "  tem = \"'\"+item+\"'\"\n",
        "  print(isOnlyThatKeyword(tem))\n",
        "\n",
        "#왜 안나올까?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfiBc_ksyA48",
        "outputId": "234f5ec2-be6c-4861-cab3-9b823c372407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['저번대선 홍이 문 개털었는데? 탄핵때문에 진거야', 0]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# 홍, 는데 모두 각 키워드만으로 매칭되는 실제 폭언, 성희롱 문장이 없다네요\n",
        "# 즉, 사전에서 해당 두 키워드를 즉시 삭제해도, 무관하다는 뜻입니다\n",
        "# 이렇게 경우의 수를 전부 계산해서 사전 csv 에서 키워드를 삭제하면 됩니다\n",
        "# 단, 유의할 점은 이렇게 사전에서 키워드를 삭제하면, 다른 문장 검증시 갱신된 사전을 바탕으로 진행해야 합니다 --> 그냥 하면 미세한 확률로 에러가 날 수도! ㅎ\n",
        "\n",
        "# 그래서 두번째 방법을 알려드립니다\n",
        "\n",
        "print(test_set[171])\n",
        "#원랜 일반 문장인데\n",
        "print(predict(test_set[171][0]))\n",
        "#폭언 문장으로 본다 텍스트 매칭에서 \n",
        "#폭언 문장으로 가야 폭언으로 봄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_JKsXtKOyu0R"
      },
      "outputs": [],
      "source": [
        "# 5. 문장 각색\n",
        "\n",
        "# 예를 들어, 위의 171번 문장에서 문제가 되는게, [홍, 는데] 두개니까\n",
        "example = \"저번대선 김이 문 이김, 탄핵때문에 진거야\"\n",
        "# 이런식으로 홍,는데 를 없애서 회피하듯이 문장을 다시 짜는 겁니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "q2Zq5nKQzI7e"
      },
      "outputs": [],
      "source": [
        "# 해당 문장은 문제가 없는지 검증해볼까요\n",
        "\n",
        "def validation(sentence):\n",
        "  morphs = kiwi.tokenize(example)\n",
        "  flag = False\n",
        "  for morph in morphs:\n",
        "    if morph.form in abuse_list or morph.form in sexual_list:\n",
        "      print(morph.form)\n",
        "      flag = True\n",
        "  \n",
        "  if flag == False:\n",
        "    print(\"문장 각색 완료\")\n",
        "  else:\n",
        "    print(\"위의 키워드가 문제가 됩니다. 문장을 다시 각색하세요!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXQJ_DbrzjoQ",
        "outputId": "0e02a0da-d82d-4de6-9e3c-4516159e12ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "김\n",
            "이기\n",
            "위의 키워드가 문제가 됩니다. 문장을 다시 각색하세요!\n"
          ]
        }
      ],
      "source": [
        "validation(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t01Q7PBC0U57",
        "outputId": "59cffda0-87e6-4876-8287-2ad6c04236d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# 각색 결과 또 문제가 생기네여 ㅋㅋㅋㅋㅋㅋㅋㅋ\n",
        "# 그럼 이번에는 이렇게 바꿀게여\n",
        "example = \"저번대선 윤이 문 개텀, 탄핵때문에 진거야\"\n",
        "validation(example)\n",
        "predict(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juh7WsO9qclN"
      },
      "source": [
        "문장검색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zne9eMNEn7OP",
        "outputId": "7afde984-4a18-4cde-c68b-e9252c2d63e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['걔네 이름부터 정 떨어짐 ㅋㅋ 평생 땅 치고 후회했으면', 0]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(test_set[104])\n",
        "print(predict(test_set[104][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz-Zb8xQsa1N",
        "outputId": "fbad9e86-15e4-483c-bb18-8260805428ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "좋\n",
            "보\n",
            "위의 키워드가 문제가 됩니다. 문장을 다시 각색하세요!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "#이건 포기\n",
        "example = \"걔네 이름부터 감명 없음 ㅋㅋ 평생 좋은 성과 내나 눈여겨 본다\"\n",
        "validation(example)\n",
        "predict(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 cell만 쓴다"
      ],
      "metadata": {
        "id": "X2F9S0DZVwv9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJI0tFsDtK7i",
        "outputId": "ca70d600-98df-49fa-cdd2-db71190fe013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['형국을 알아야 너같은 놈한테도 짓밟히지 않을 거 아냐', 0]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "num = 577\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD2xPjP3tQtW",
        "outputId": "7683db35-ef10-4868-aa5c-1835490451e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "example = \"형국을 깨우치면 니부터도 안 짓밟혀\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다시 추가 3개"
      ],
      "metadata": {
        "id": "JGJ0rCPQ6icB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = 104\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9wcgjVO6DfL",
        "outputId": "a5456727-7d66-4c43-cda0-2536a6b71684"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['걔네 이름부터 정 떨어짐 ㅋㅋ 평생 땅 치고 후회했으면', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"걔네 이름부터 킹받노 \"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wHnIEaq6Gs3",
        "outputId": "a6eb111a-c7ae-4acf-9be9-d7519080febf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 135\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFXmCZTW6g9O",
        "outputId": "1b35e463-5de6-4a9a-fdfc-06177a18cab3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안락사는 죽음을 존엄하게 맞을 수 있게 돕자는 건데.', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"강아지 유기하면 주홍글씨 남기든가 해\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ay_fUmh6nB-",
        "outputId": "241ea17f-ac49-421a-f765-677516a92a9c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 246\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC90fuiX7yOL",
        "outputId": "084cca4f-95b2-4e3a-c129-421c5abdb8aa"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['너 아이스크림 먹고 싶다며 내가 배민으로 시켜줄까?', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"연지곤지 분칠할라\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBu5VHE37zbo",
        "outputId": "193942bd-e95d-424d-c815-f0c39f41495e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3jvIh8xvZjo",
        "outputId": "c44a4f46-3d42-41c4-8e2d-5bc5a6de1353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['걔는 그 사람 잘생겼다고 지지하는 빠돌이더라', 0]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "num = 1521\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEXK6ky7wfQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876605ab-3e66-40fe-d656-d3e3524570cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "example = \"걔는 걔 훈훈하다고 사랑하는 빠돌이야\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 2469\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fc9uRtIaZri",
        "outputId": "00bb8eff-7bf8-4766-db4b-04cb5ef97b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['가만히 있어 이 벌레야.', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"가만히 있어 이 생물아.\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LItYBIWyacle",
        "outputId": "7de69635-9723-4231-8c8e-c4e9ad45832b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 2914\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ2CK4H5d-Zq",
        "outputId": "f01b7399-e525-48a5-e129-f309a9e4dbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['너 진짜 핑프같아', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"너 매우 핑프같아\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBYryZkreEQQ",
        "outputId": "54a16554-5dc7-4186-afa2-d84b5860b9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 2909\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmBwffIcexPG",
        "outputId": "d4ced303-fabc-467f-a079-f157dad90bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['야 걔네도 나름 개과천선해서 열심히 한 거야', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"야 걔네도 나름 개과천선해 열심히 한 거야\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akkwnblXez-_",
        "outputId": "44f36a6b-85f4-4404-b5b4-4fa725e163ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 2474\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4cr1rhpfQws",
        "outputId": "27613c7a-2e4c-41a5-d97b-aa2d8a0c3033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['진짜 철딱서니가 없어도 너무 없어', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"무척 철딱서니가 없지만 너무 없어\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmP5pkLTfUUu",
        "outputId": "2c08e0a8-3f93-417b-9408-1f6f2f88e8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 2742\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2swFY3Mfsbo",
        "outputId": "a404d467-d65c-48e8-edbe-62ab71621bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['도련님 못하겠으면 아이 삼촌이라고 해라', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"도련님 못하면 아이 삼촌이라고 해\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QvnouLefvbK",
        "outputId": "a64cda4d-9be0-405f-e6f9-0e4a8c827edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 3626\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ODUDC-hKaS",
        "outputId": "66ec1552-613b-45cf-d01f-d8bd2e8c35af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나도 흑인으로 살 바엔 그냥 다음 생 도박 할 듯', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"나도 황인일 바엔 그냥 다음 생 다시 할 듯\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW1u3xLLhMUN",
        "outputId": "c3d7cfc2-01fe-402e-aad1-7a61b41f6bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 3600\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaeXqzTBikG4",
        "outputId": "eeb98919-7734-4057-b1f5-78d8b89cb6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1000원에 반찬 세 개랑 밥, 국 나오면 혜자 아님?', 0]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"1000원에 반찬 다섯에 찌개는 혜자 아님?\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "550cvKxiioDj",
        "outputId": "e101041f-7dd5-42ac-8582-9f0063a6d916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#폭언 set에서 171, 577, 1521, 2469, 3600, 2914, 2909, 2474, 2742, 3626 총 10개 수정"
      ],
      "metadata": {
        "id": "xlf4-wUxpXRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlf1KhWiVyvc",
        "outputId": "6cda34f4-822a-4bf9-f5d5-d4f98b1d267f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['짱개',\n",
              " '어도',\n",
              " '정',\n",
              " '떨어지',\n",
              " '땅',\n",
              " '치',\n",
              " '안락사',\n",
              " '죽음',\n",
              " '맞',\n",
              " '자는',\n",
              " '홍',\n",
              " '먹',\n",
              " '시키',\n",
              " '멍',\n",
              " '때리',\n",
              " '감방',\n",
              " '사기치',\n",
              " '먹',\n",
              " '지능',\n",
              " '크',\n",
              " '넘',\n",
              " '장애',\n",
              " '사람',\n",
              " '들',\n",
              " '잘',\n",
              " '잘',\n",
              " '낳',\n",
              " '어야지',\n",
              " '놈',\n",
              " '놈',\n",
              " '놈',\n",
              " '밑',\n",
              " '빠지',\n",
              " '때리',\n",
              " '치',\n",
              " '어라',\n",
              " '가족',\n",
              " '당하',\n",
              " '사형',\n",
              " '감',\n",
              " '들',\n",
              " '내버리',\n",
              " '두',\n",
              " '차리',\n",
              " '줄',\n",
              " '닥치',\n",
              " '이나',\n",
              " '시체',\n",
              " '토막',\n",
              " '들',\n",
              " '도살',\n",
              " '시키',\n",
              " '버리',\n",
              " '좋',\n",
              " '사살',\n",
              " '부모',\n",
              " '죽',\n",
              " '멍청하',\n",
              " '짓',\n",
              " '개빡',\n",
              " '치',\n",
              " '사람',\n",
              " '지지',\n",
              " '팔',\n",
              " '자식',\n",
              " '말',\n",
              " '미개',\n",
              " '들',\n",
              " '헛',\n",
              " '소리',\n",
              " '조지',\n",
              " '걷',\n",
              " '떨어지',\n",
              " '뒤지',\n",
              " '뒤지',\n",
              " '이나',\n",
              " '뜯기',\n",
              " '좋',\n",
              " '쓰레기',\n",
              " '들',\n",
              " '참교육',\n",
              " '디지게',\n",
              " '가지',\n",
              " '똥',\n",
              " '나오',\n",
              " '미치',\n",
              " '벌레',\n",
              " '돌멩이',\n",
              " '던지',\n",
              " '어도',\n",
              " '부모',\n",
              " '자식',\n",
              " '장애인',\n",
              " '취급',\n",
              " '당하',\n",
              " '사회',\n",
              " '가난',\n",
              " '부모',\n",
              " '들',\n",
              " '좋',\n",
              " '낳',\n",
              " '쓰',\n",
              " '개',\n",
              " '빡',\n",
              " '치',\n",
              " '어라',\n",
              " '엄마',\n",
              " '차리',\n",
              " '차리',\n",
              " '먹',\n",
              " '식',\n",
              " '대',\n",
              " '모가지',\n",
              " '부리',\n",
              " '말',\n",
              " '들',\n",
              " '라도',\n",
              " '양심',\n",
              " '죄',\n",
              " '인정',\n",
              " '어야지',\n",
              " '들',\n",
              " '사회',\n",
              " '나오',\n",
              " '개',\n",
              " '반',\n",
              " '살',\n",
              " '면서',\n",
              " '맞',\n",
              " '살인',\n",
              " '사형',\n",
              " '시키',\n",
              " '개',\n",
              " '밥',\n",
              " '국',\n",
              " '나오',\n",
              " '흑인',\n",
              " '살',\n",
              " '도박',\n",
              " '주제',\n",
              " '감',\n",
              " '놓',\n",
              " '어라',\n",
              " '사람',\n",
              " '들',\n",
              " '불',\n",
              " '지르',\n",
              " '괴물',\n",
              " '길거리',\n",
              " '나오',\n",
              " '좋',\n",
              " '예쁘',\n",
              " '아가',\n",
              " '굶기',\n",
              " '죽이',\n",
              " '말',\n",
              " '감자']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "temp_abuse_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4sTOKN80inA"
      },
      "outputs": [],
      "source": [
        "# 이번에는 문제가 없네여!\n",
        "# 그럼 해당 문장을 csv에서 바꿔주면 됩니다!\n",
        "\n",
        "# ㅎㅎ.. 복잡하죠 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n",
        "# 사전 각색 vs 문장 각색 비교하면, 아무래도 문장 각색이 신경쓸게 덜합니다  \n",
        "# 사전 각색이 귀찮은 이유는 사전에서 키워드를 제거하고 갱신된 사전을 다시 코랩으로 로드하고 다음 문장을 작업해야하기 때문입니다 (갱신 안하고 작업하면, 정확한 산출이 불가)\n",
        "\n",
        "# 그래서 왠만하면 적절하게 문장을 각색하고 검증하는 방식을 이용하면 됩니다!\n",
        "# 하지만, 제가 사용한 예시 문장처럼 '는데' 이런 말도안되는 키워드가 문제가 되면, 아무래도 사전 자체에서 삭제하는 편이 좋겠죠?\n",
        "# --> 판단은 알아서!, 사전 각색이 귀찮으면, 전부 문장 각색으로 하셔도 됩니다~ --> 제가 봤을 때는 '는데'만 삭제하고 이후에는 그냥 문장 각색으로 빠르게 해도 될 것 같네요\n",
        "\n",
        "# 위의 알고리즘에서 3번부터는 폭언으로 잘 못 판별한 문장으로만 진행 중입니다\n",
        "# 그러니까 성희롱으로 잘 못 판별한 문장에서도 각색이 필요해요!\n",
        "# 성희롱 코드는 아래에서 확인!\n",
        "\n",
        "# 이렇게 30문장 정도만 추가로 거르면 좋을 것 같아요! 고생하세여! ㅎ  (죄송해요ㅜ 많으면 많을 수록 좋을 것 같아요..ㅎ)\n",
        "# 아 각색 과정에서, 사전을 수정했으면, 어떤 키워드를 수정했는지\n",
        "# 문장을 수정했으면 어떤 문장을 수정했는지 인덱스 정보를 기록해두세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuK7vOi62o5R",
        "outputId": "2b7d4f68-5fbc-4c69-9799-7299d405d928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25, ['성관계', '끝나', '었', '는데', '도', '계속', '이쁘', '다고', '하', '어', '주', '고', '이름', '이랑', '전화', '번호', '도', '알리', '어', '주', '네', '.'], ['알리'], ['성관계', '는데', '주', '주']]\n",
            "\n",
            "[142, ['애견', '카페', '가', '었', '는데', '나', '의', '옆', '에서', '강아지', '들', '짝짓기', '하', 'ᆷ', '..'], ['들'], ['는데', '옆', '들', '짝짓기']]\n",
            "\n",
            "[155, ['새', '거', '도', '아니', 'ᆫ', '스타킹', '으로', '포장', '되', '어', '있', '음'], ['새'], ['스타킹']]\n",
            "\n",
            "[159, ['러시아', '여자', '들', '이', '리얼', '예쁘', 'ᆷ'], ['여자', '들', '예쁘'], ['여자', '들', '예쁘']]\n",
            "\n",
            "[285, ['잘', '찍', '고', '오', '었', '지', '~', '근데', '가슴', '이', '작', '어서', '곤란', '하', '었', '음'], ['잘', '찍', '가슴', '작'], ['잘', '찍', '가슴', '작', '어서']]\n",
            "\n",
            "[301, ['무슨', '대단', '하', 'ᆫ', '집', '아들', '이', 'ᆫ', '줄', '알', '었', '더니', '남자', '개털', '이', '구만'], ['아들', '줄'], ['무슨', '줄', '알', '남자']]\n",
            "\n",
            "[569, ['넘', '귀엽', '지', '않', '냐', '??', '저', '정도', '이', '면', '강아지', '탈', '을', '쓰', 'ᆫ', '사람', '아니', 'ᆯ까', 'ㅋㅋ'], ['넘', '탈', '쓰', '사람'], ['냐', '탈', '쓰']]\n",
            "\n",
            "[1091, ['ㄹㅇ', '거기', '에', '돈', '쓰', '는', '아재', '가', '얼마나', '많', '은데'], ['거기', '쓰', '아재', '많'], ['거기', '돈', '쓰', '많']]\n",
            "\n",
            "[1102, ['평소', '에', '덜렁대', '었', '잖아', '.', '너', '도', '알', '면서'], ['면서'], ['알']]\n",
            "\n",
            "[1168, ['2', '년', '동안', '구르', '고', '박', '고', '얻', '어', '터지', 'ᆫ', '적', '없', '으면', '입', '딱', '다물', '어라'], ['년', '구르', '박', '터지', '적', '입', '딱', '다물', '어라'], ['년', '구르', '박', '터지', '적', '으면', '입', '딱', '어라']]\n",
            "\n",
            "[1219, ['달고나', '가', '뭐', '가', '맛있', '다고', '난리', '들', '이', '야', ';'], ['난리', '들'], ['맛있', '들']]\n",
            "\n",
            "[1389, ['쟤', '가', '아직', '도', '뮤지컬', '쪽', '에서', '는', '몸값', '탑', '이', '야', '.'], [], ['몸값', '탑']]\n",
            "\n",
            "[1436, ['청국장', '좋아하', '는', '애', '들', '보', '면', '아재', '입맛', '이', 'ᆫ', '거', '같', '은데', '좀', '그렇', '다'], ['들', '아재'], ['들', '보', '좀']]\n",
            "\n",
            "[1642, ['우리', '겜방', '가', '어서', '겜', '한판', '만', '하', '자'], [], ['우리', '어서', '한판', '자']]\n",
            "\n",
            "[1750, ['환자', '덮치', '는', '의사', '가', '의사', '이', '냐'], ['환자'], ['덮치', '냐']]\n",
            "\n",
            "[1761, ['막', '줍', '다가', '잡히', '어', '가', '는', '거', '아니', '야', '?', 'ㅋㅋ'], ['막'], ['막', '다가']]\n",
            "\n",
            "[2078, ['머', '가', '있', '어야', '적을거잔아요', '나', 'ᆫ', '장점', '없', '어요', '.'], ['머'], ['머', '어야']]\n",
            "\n",
            "[2097, ['가슴', '크', 'ᆫ', '여자', '들', '크로스', '백', '메', '는', '거'], ['가슴', '크', '여자', '들', '메'], ['가슴', '크', '여자', '들']]\n",
            "\n",
            "[2115, ['이거', '보', '면', '배꼽', '빠지', 'ᆷ'], ['빠지'], ['보', '배꼽', '빠지']]\n",
            "\n",
            "[2267, ['분명히', '해', '쨍쨍', '하', 'ᆯ', '거', '이', '라', '하', '었었', '는데'], [], ['해', '는데']]\n",
            "\n",
            "[2758, ['우리', '어리', 'ᆯ', '때', '는', '피카츄', '진짜', '많이', '먹', '었', '는데', 'ㅎ'], ['어리', '먹'], ['우리', '어리', '때', '진짜', '먹', '는데']]\n",
            "\n",
            "[2765, ['니', '딸', '도', '아니', 'ᆫ데', '40', '대', '를', '만나', '어', '들', 'ᆫ', '50', '대', '를', '만나', '든', '왜', '열', '폭', '이', '냐'], ['딸', '대', '들', '대', '폭'], ['딸', '대', '만나', '들', '대', '만나', '폭', '냐']]\n",
            "\n",
            "[2807, ['우리', '나라', '는', '아직', '야매', '가', '많이', '서', '글', '치', '멋지', 'ᆫ데', '?'], ['서', '치'], ['우리', '서', '글']]\n",
            "\n",
            "[3074, ['저쪽', '은', '엉덩이', '수술', '많이', '하', 'ᆫ대', '요'], [], ['엉덩이', '수술']]\n",
            "\n",
            "[3177, ['왜', '화', '를', '내', '어', '~', '몰카', '야', 'ㅋㅋ'], ['화', '몰카'], ['화', '몰카']]\n",
            "\n",
            "[3341, ['아', '뭐', '이', '라는', '거', '이', '야', '얘', '네', '츄르', '에', '환장', '하', '어'], ['환장'], []]\n",
            "\n",
            "[3347, ['여자', '친구', '를', '사귀', '면', '헤프', '게', '놀', '는', '거', '이', '야', '?'], ['여자', '사귀'], ['여자', '친구', '헤프', '놀']]\n",
            "\n",
            "[3375, ['남', '의', '집', '복도', '계단', '에서', '어떤', '놈', '이', '딸', '치', '다가', 'cctv', '에', '찍히', '었', '대'], ['놈', '딸', '치', '찍히', '대'], ['딸', '다가', '대']]\n",
            "\n",
            "[3388, ['누구', '가', '자꾸', '나', '의', '속옷', '훔치', '어', '가', 'ᆫ다', '누구', '이', '냐'], [], ['속옷', '냐']]\n",
            "\n",
            "[3532, ['그냥', '다', '밈', '이', '지', '설마', '진짜', '이', '겠', '냐'], [], ['진짜', '냐']]\n",
            "\n",
            "[3534, ['나', 'ᆫ', '단벌', '숙녀', '이', '야', '그러', 'ᆯ', '수', '있', '자', 'ᆫ', '아'], [], ['자']]\n",
            "\n",
            "[3749, ['사자', '의', '저', '멋지', 'ᆫ', '갈기', '들', '이', '더', '멋지', '게', '보이', '는', '거', '같', '애'], ['사자', '갈기', '들', '보이'], ['갈기', '들', '보이']]\n",
            "\n",
            "[3854, ['오키나와', '여행', '가', '었', '을', '때', '까마귀', '가', '가방', '지퍼', '열', '고', '빵', '꺼내', '어', '가', '더라', 'ㅋㅋ'], ['빵', '꺼내'], ['때', '빵', '더라']]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 부록. 성희롱으로 잘 못 판별한 문장\n",
        "\n",
        "for rows in moral_sexual_morphs:\n",
        "  abuse_matched = list()\n",
        "  sexual_matched = list()\n",
        "\n",
        "  for morph in rows[1]:\n",
        "    if morph in abuse_list:\n",
        "      abuse_matched.append(morph)\n",
        "    if morph in sexual_list:\n",
        "      sexual_matched.append(morph)\n",
        "\n",
        "  rows.append(abuse_matched)\n",
        "  rows.append(sexual_matched)\n",
        "\n",
        "for rows in moral_sexual_morphs:\n",
        "  print(rows)\n",
        "  print()\n",
        "\n",
        "# 성희롱으로 잘못 분류된 문장 모두에 대해 (test_set 인덱스, 문장 형태소, 폭언 사전 매칭 형태소, 성희롱 사전 매칭 형태소)\n",
        "# 제가 봤을 때는 1102, 2078, 2267 요런 문장들이 각색하기 편해 보이네여"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 3532\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEDzbWdXq1zu",
        "outputId": "487d6f03-24d2-462d-bec5-32514ea4c006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['그냥 다 밈이지 설마 진짜겠냐', 0]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"그냥 다 밈이지 설마 리얼이겠어?\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzYmUxoGq_SZ",
        "outputId": "57daf5b1-a096-4f4a-95ef-634441795e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 3534\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAUVtABUrhkK",
        "outputId": "5971defe-6fce-48ee-b292-23c2d0768726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['난 단벌 숙녀야 그럴수있잔아', 0]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"난 단벌 숙녀야 그럴수있잖아\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuKdz-5NroUj",
        "outputId": "d6ca23b2-63f1-4b66-b839-60b6086dc7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 2267\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzMzodJ4rs0n",
        "outputId": "17a4e2f4-7481-46f4-863f-fef587cfac97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['분명히 해 쨍쨍할거라 했었는데', 0]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"분명히 쨍쨍할거라 했었다\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtZiDz1fsQUz",
        "outputId": "de24ae85-69d4-4771-a323-f332465b0066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 1102\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgzzZyxvtA5g",
        "outputId": "8b876575-3274-4bc1-ccbf-c9d21cb23111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['평소에 덜렁댔잖아. 너도 알면서', 0]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"평소에 덜렁댔잖아. 너도 동의하잖아\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elI--OEmtDLd",
        "outputId": "d3130493-e8ad-476e-e0e0-cb35eca21da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 2807\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg79OupWuD2R",
        "outputId": "91e24fee-89ed-4af3-b1c9-ccf8e68c83ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['우리나라는 아직 야매가 많이서 글치 멋진데?', 0]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"대한민국은 아직 야매가 다소 있지만 멋진데?\"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTkdm6fJuU-5",
        "outputId": "fd15763e-2229-4962-d78f-18e03e6534d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 1750\n",
        "print(test_set[num])\n",
        "label = predict(test_set[num][0])\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpL-Bgo7vYS1",
        "outputId": "9bf432ba-d53e-4594-fb90-b11986d08077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['환자 덮치는 의사가 의사냐', 0]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"간음하는 의사가 의사야? \"\n",
        "validation(example)\n",
        "print(predict(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vComDiFvcpD",
        "outputId": "af78e48a-9d2f-4ca7-ca1f-2454fac5d4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 각색 완료\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#성희롱 문장은 1102, 1750, 2807, 2267, 3534, 3532 (총 6개)"
      ],
      "metadata": {
        "id": "MwmFKKqdvnT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 성희롱, 폭언 두 개 합쳐 거르는게 16개입니다 "
      ],
      "metadata": {
        "id": "EgH1Cx6ZEyrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}