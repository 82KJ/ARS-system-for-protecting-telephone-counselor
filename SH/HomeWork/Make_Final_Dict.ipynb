{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7QtMgW5chSj"
      },
      "source": [
        "# 1. 준비\n",
        "- cpu 환경은 너무 느려서 코랩에서 부탁드립니다 ㅎ\n",
        "- 꼭 GPU 가속이 켜져있는지 확인해주세요!\n",
        "- 추가로, 구글 드라이브에 학습 데이터, 사전 데이터, 모델을 모두 로드해 주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSBlEIPocGRu",
        "outputId": "16615ab3-26b2-470f-902b-58145278ad6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-eofaoxhr\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-eofaoxhr\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonnlp<=0.10.0,>=0.6.0\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
            "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n",
            "  Downloading torch-1.10.1-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.22.4)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (23.3.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.20.3)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.19.0,>=1.18.18\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.34)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.0)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.27.1)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.10.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.10.31)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting urllib3<1.26,>=1.20\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.1.1)\n",
            "Building wheels for collected packages: kobert, gluonnlp, sacremoses\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15705 sha256=3a9acd8dcb58099fc542b98b7e508fd1d83c43b02b9d4dba5b9071bbec2bb488\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h_01ejp2/wheels/0b/20/d8/031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp39-cp39-linux_x86_64.whl size=680540 sha256=082a9e708daf36210e2d89ca0a25ff6c8793aae9a2314d96a95a53067be41867\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/17/70/b257bc53879a458c4bfcc900e89271aa8b4f19366a54bd2455\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=d976828ef9d6b0559efd150585fea89a9e8ce861d49dcc04a11130de5f8b017a\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built kobert gluonnlp sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, urllib3, torch, sacremoses, onnxruntime, jmespath, graphviz, gluonnlp, botocore, s3transfer, mxnet, huggingface-hub, transformers, boto3, kobert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "XB4Qc7ArjOdD",
        "outputId": "6c5900e9-64c1-43ed-e313-f52462c40710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kiwipiepy\n",
            "  Downloading kiwipiepy-0.15.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from kiwipiepy) (1.22.4)\n",
            "Collecting kiwipiepy-model~=0.15\n",
            "  Downloading kiwipiepy_model-0.15.0.tar.gz (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: kiwipiepy-model\n",
            "  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.15.0-py3-none-any.whl size=30602642 sha256=ac042ffdb003fdf969c1d2497546d7bd41f3716d724f783d19c8dcf4bf8234c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/34/31/a1682c8249bf2ba89a29498d62c35c08ceb116537b4530d93e\n",
            "Successfully built kiwipiepy-model\n",
            "Installing collected packages: kiwipiepy-model, dataclasses, kiwipiepy\n",
            "Successfully installed dataclasses-0.6 kiwipiepy-0.15.0 kiwipiepy-model-0.15.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install kiwipiepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPyx4PBhcwtD",
        "outputId": "1cf093ce-3317-4ddc-dcbc-35109c90a4af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/종합설계\n"
          ]
        }
      ],
      "source": [
        "# 드라이브 위치를 알맞게 조정하세요!\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/종합설계 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl4ydJ25do1d",
        "outputId": "4faefadd-b8d2-4876-d3b6-2de186579f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cached model. /content/drive/MyDrive/종합설계/.cache/kobert_v1.zip\n",
            "using cached model. /content/drive/MyDrive/종합설계/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjhhqi-DeDXQ"
      },
      "source": [
        "# 2. 데이터 준비\n",
        "- 여기서는 training_dataset.csv를 활용해서 train, test set을 준비하게 됩니다!\n",
        "- 제가 디렉토리에 같이 넣어놀테니까 이거 활용하세요! --> 구글 드라이브로 옮기기\n",
        "- 추가로, 사전도 미리 준비해 놓겠습니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tO8e36aAd675"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"training_dataset.csv\")\n",
        "\n",
        "data_list = list()\n",
        "for sen, lab in zip(data[\"0\"], data[\"1\"]):\n",
        "  data_list.append([sen,lab])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVIAuKD6efZO",
        "outputId": "b12a2657-579e-4cf9-87f5-a32f455dd1aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['그 동안 보빨러들이 얼마나 잘 해줬겠어ㅋㅋㅋ', 2]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(data_list, test_size=0.1, random_state=0) # train : test = 9:1\n",
        "train_set[0]\n",
        "\n",
        "# 여기서 만들어진 test_set을 중점으로 활용하시면 됩니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEly2gEjehgB",
        "outputId": "56bd090b-deb0-4e11-a8be-4b5f27f14934"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['인정 쟤네 목소리 쫌 심각하게 족같아', 1],\n",
              " ['나 알바하는 곳 너무 장사 잘돼서 힘들어.', 0],\n",
              " ['아슬아슬~ 정말 프로답게 벗어 주네.', 2]]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set[:3]\n",
        "\n",
        "# 아래를 보면 알겠지만, 각 row마다 0번이 문장 데이터, 1번이 라벨링 데이터입니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HW9glruxeoSj"
      },
      "outputs": [],
      "source": [
        "abuse = pd.read_csv(\"폭언사전.csv\")\n",
        "sexual = pd.read_csv(\"성희롱사전.csv\")\n",
        "\n",
        "# 사전은 아래에서 쓰일 거라서 미리 로드합니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "kHMAWMdJfL6e",
        "outputId": "94803244-7868-43be-8b95-d176ae367e84"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7284f160-6555-4732-bc38-26d75251203c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>가난</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>가난뱅이</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>가두</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7284f160-6555-4732-bc38-26d75251203c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7284f160-6555-4732-bc38-26d75251203c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7284f160-6555-4732-bc38-26d75251203c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0     0\n",
              "0           0    가난\n",
              "1           1  가난뱅이\n",
              "2           2    가두"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abuse.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "yaHt_zcEfOhh",
        "outputId": "8ea9491d-3a40-41e6-98f3-b210d3670ba4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0cf0f852-44e3-4cf2-a1f9-968d07cab54f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>19금</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>69자세</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cf0f852-44e3-4cf2-a1f9-968d07cab54f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cf0f852-44e3-4cf2-a1f9-968d07cab54f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cf0f852-44e3-4cf2-a1f9-968d07cab54f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0     0\n",
              "0           0   19금\n",
              "1           1    69\n",
              "2           2  69자세"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sexual.head(3)\n",
        "\n",
        "# 사전이 dataframe으로 로드되면서, 컬럼명[Unnamed:0, 0]으로 자동 지정되었어요!\n",
        "# 여기서 '0' col을 사용하면 되겠죠?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OLibdmZAfhR0"
      },
      "outputs": [],
      "source": [
        "abuse_list = list()\n",
        "sexual_list = list()\n",
        "\n",
        "for rows in abuse[\"0\"]:\n",
        "  abuse_list.append(rows)\n",
        "\n",
        "for rows in sexual[\"0\"]:\n",
        "  sexual_list.append(rows)\n",
        "\n",
        "# dataframe로 load한 사전이 사용하기 불편할 수도 있으니, 리스트 형식으로도 저장합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXMNhsVlf7E-",
        "outputId": "22b0899a-4a3e-4b54-f2d8-4135007ae165"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['가난', '가난뱅이', '가두'], ['19금', '69', '69자세'])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abuse_list[:3], sexual_list[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXGKWs2GgBxG"
      },
      "source": [
        "# 3. 코버트 모델 로드\n",
        "- 여기서는 저희가 만든 \"kobert_classifier.pth\"을 로드해서 모델을 준비합니다\n",
        "- 모델은 가지고 계시죠? 그거 구글 드라이브에 로드해서 사용하시면 됩니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtKReUMYf-KY",
        "outputId": "e3d87ebf-f3cb-4d71-b80b-070198c4088f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cached model. /content/drive/MyDrive/종합설계/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "\n",
        "    # sentence , label data를 BERT의 입력값에 맞게 변환하는 transformer를 생성\n",
        "    transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
        "\n",
        "    ## 생성한 transformer로 sentence를 변환하여 저장\n",
        "    self.sentences = [transform([data[sent_idx]]) for data in dataset]\n",
        "    self.labels = [np.int32(data[label_idx]) for data in dataset]\n",
        "  \n",
        "  def __getitem__ (self, i):\n",
        "    return (self.sentences[i] + (self.labels[i], )) # 각 index에 맞는 item 반환 진행 --> 왜 이런 형태인지는 잘 모르겠음\n",
        "  \n",
        "  def __len__(self):\n",
        "    return (len(self.labels))\n",
        "\n",
        "# Parameter setting 진행\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n",
        "\n",
        "# Kobert 모듈에서 제공하는 get_tokenizer와 vocab를 활용해 tokneizer를 구성한다\n",
        "tokenizer = get_tokenizer() \n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(train_set, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(test_set, 0, 1, tok, max_len, True, False)\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert, hidden_size = 768, num_classes=3, dr_rate=None, params=None):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.dr_rate = dr_rate\n",
        "\n",
        "    ## classifier는 선형 회귀 모델로 구성 (input size = 768, output size = 3 (label이 3개로 구성))\n",
        "    self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    ## overfitting 방지를 위한 dropout 비율 설정\n",
        "    if dr_rate:\n",
        "      self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "  # attention mask sequence를 구성해주는 함수 --> padding이 아닌 영역을 0에서 1로 변경\n",
        "  def gen_attention_mask(self, token_ids, valid_length):\n",
        "    attention_mask = torch.zeros_like(token_ids)\n",
        "    for i,v in enumerate(valid_length):\n",
        "      attention_mask[i][:v] = 1\n",
        "    \n",
        "    return attention_mask.float()\n",
        "  \n",
        "  # bert + classifier를 관통하는 forward 연산 진행\n",
        "  def forward(self, token_ids, valid_length, segment_ids):\n",
        "\n",
        "    # attention_mask 계산\n",
        "    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "    # bert에 input 투입, 변수명이 pooler인거 보니 출력 embedding에 mean pooling 적용한 값이지 않을까 추측\n",
        "    _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "\n",
        "    # dropout 비율이 존재한다면, dropout 적용\n",
        "    if self.dr_rate:\n",
        "        out = self.dropout(pooler)\n",
        "\n",
        "    # classifier 진행\n",
        "    return self.classifier(out) \n",
        "\n",
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "model_state_dict = torch.load(\"kobert_classifier.pth\", map_location=device)\n",
        "model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDFnsU6tgty1"
      },
      "source": [
        "# 4. test_set 예측 라벨 산출\n",
        "- 이제 3870개의 test 문장을 모델에 투입해서, 예측된 라벨링을 얻을 거예요!\n",
        "- 결과 라벨은 y_hat에 저장할게요~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mfT_dl6Cgm2s"
      },
      "outputs": [],
      "source": [
        "def predict(predict_sentence):\n",
        "  # 1. data set 구성 (문장, 라벨)\n",
        "  data = [predict_sentence, '0']\n",
        "  dataset_another = [data]\n",
        "\n",
        "  # 2. data를 bert의 입력에 맞게 변환하기\n",
        "  another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "  test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=0)\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "      token_ids = token_ids.long().to(device)\n",
        "      segment_ids = segment_ids.long().to(device)\n",
        "      valid_length= valid_length\n",
        "      label = label.long().to(device)\n",
        "\n",
        "      # 모델 forward 연산 진행\n",
        "      out = model(token_ids, valid_length, segment_ids)\n",
        "      \n",
        "      # torch out -> numpy 형식으로 변환\n",
        "      logits = out[0].detach().cpu().numpy()\n",
        "\n",
        "      return np.argmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fD4fCOmuhGjL"
      },
      "outputs": [],
      "source": [
        "y_hat = list()\n",
        "\n",
        "for rows in test_set:\n",
        "  labels = predict(rows[0])\n",
        "  y_hat.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKNKsW_OhPLm",
        "outputId": "080337a1-2104-4c0a-934a-97a84eac1470"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 0, 2]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat[:3]\n",
        "\n",
        "# 이게 저희 모델로 예측한 라벨 값들 입니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GZZfB9Szhg7V"
      },
      "outputs": [],
      "source": [
        "y = list()\n",
        "\n",
        "for rows in test_set:\n",
        "  y.append(rows[1])\n",
        "\n",
        "# test_set은 [문장, 실제 라벨값]의 구조라서 실제 라벨값만 따로 y에 저장했어요 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-fwsxlNhkpa",
        "outputId": "04c2dc81-08f6-4566-f442-1cc26b5d9dc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 0, 2]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[:3]\n",
        "\n",
        "# 요건 실제 라벨값 입니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq-6AuNwiM7i",
        "outputId": "81989e1f-157d-4d4a-acc8-cfb083ab5a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===Confusion Matrix===\n",
            "[[1912   54   33]\n",
            " [  62  906   43]\n",
            " [  55   42  763]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "cm = confusion_matrix(y, y_hat)\n",
        "print(\"===Confusion Matrix===\")\n",
        "print(cm)\n",
        "\n",
        "# y, y_hat으로 confusion matrix을 산출한 결과입니다\n",
        "# row는 실제 라벨\n",
        "# col은 예측 라벨\n",
        "# 즉, 0행은 실제 일반 문장 1999개에서 (1912 + 54 + 33) 1912개는 올바르게 평가, 54개는 폭언으로 잘못 산출, 33개는 성희롱으로 잘못 산출 했다는 것을 의미합니다\n",
        "# 우리의 목표는 54와 33으로 잘못 판별된 문장들을 모델 투입 전에 사전에서 미리 거를 수 있도록, 사전, 문장 각색을 진행하는 것입니다!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2PFSS5s2jZPF"
      },
      "source": [
        "# 5. 사전 각색 or 문장 각색\n",
        "- 이제 본격적인 알고리즘 작성이 필요합니다\n",
        "- 대략적인 순서는 제가 여기에 적어둘게여\n",
        "  - y 와 y_hat을 비교하면서, y==0 인데, y_hat != 0이 아닌 문장의 인덱스를 구합니다!\n",
        "  - 이제, 구한 인덱스를 바탕으로 test_set에서 문장을 찾아서, 형태소 분해 후 저장을 합니다\n",
        "  - 저장된 형태소 리스트를 바탕으로, 각각 사전의 어떤 키워드와 매칭되는지 확인합니다!\n",
        "  - 마지막으로, 사전 각색 혹은 문장 각색 두 방법 중 하나를 선택해서 진행하시면 됩니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0Js6NrziU4B",
        "outputId": "f71b841a-2200-478a-fd34-74b52497d563"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(54, 33)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. y 와 y_hat을 비교하면서, y==0 인데, y_hat != 0이 아닌 문장의 인덱스를 구합니다! (== 실제 일반 문장인데, 폭언, 성희롱으로 잘못 예측된 문장의 인덱스)\n",
        "\n",
        "moral_abuse_idx = list() # 일반인데 폭언으로 잘못 판별된 문장의 인덱스 리스트\n",
        "moral_sexual_idx = list() # 일반인데 성희롱으로 잘못 판별된 문장의 인덱스 리스트\n",
        "\n",
        "for idx in range(len(y)):\n",
        "  if y[idx] == 0 and y_hat[idx] == 1:\n",
        "    moral_abuse_idx.append(idx)\n",
        "  elif y[idx] == 0 and y_hat[idx] == 2:\n",
        "    moral_sexual_idx.append(idx)\n",
        "\n",
        "\n",
        "# 구한 인덱스의 길이가 confusion matrix의 54, 33으로 나오겠죠?\n",
        "len(moral_abuse_idx), len(moral_sexual_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUHO8HX2kwEE",
        "outputId": "3423083e-2ca5-4169-c272-f5063d570dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[33, ['야', '짱개', '언어', '그냥', '모르', '어도', '되', '거든', '?']]\n",
            "[25, ['성관계', '끝나', '었', '는데', '도', '계속', '이쁘', '다고', '하', '어', '주', '고', '이름', '이랑', '전화', '번호', '도', '알리', '어', '주', '네', '.']]\n"
          ]
        }
      ],
      "source": [
        "# 2. 찾은 인덱스를 바탕으로 test_set에서 문장을 찾고, 형탠소 분해 후 저장해요\n",
        "from kiwipiepy import Kiwi\n",
        "kiwi = Kiwi()\n",
        "\n",
        "moral_abuse_morphs = list()    # 일반인데 폭언으로 잘못 판별된 문장의 형태소 리스트\n",
        "moral_sexual_morphs = list()   # 일반인데 성희롱으로 잘못 판별된 문장의 형태소 리스트\n",
        "\n",
        "for idx in moral_abuse_idx:\n",
        "  morphs = kiwi.tokenize(test_set[idx][0])\n",
        "  temp = list()\n",
        "  for morph in morphs:\n",
        "    temp.append(morph.form)\n",
        "  moral_abuse_morphs.append([idx,temp])\n",
        "\n",
        "for idx in moral_sexual_idx:\n",
        "  morphs = kiwi.tokenize(test_set[idx][0])\n",
        "  temp = list()\n",
        "  for morph in morphs:\n",
        "    temp.append(morph.form)\n",
        "  moral_sexual_morphs.append([idx,temp])\n",
        "\n",
        "print(moral_abuse_morphs[0])\n",
        "print(moral_sexual_morphs[0])\n",
        "\n",
        "# 출력된 결과를 보면, 짱개, 성관계 등의 형태소가 포함되어 있어서 두 문장에 대한 표현은 사실 그냥 넘어가야 합니다 ㅜ\n",
        "# 아래 문장들은 사실, 일반 문장 데이터 만들 때 제거되었어야 맞는데.. 흠.. 누군가 대충 넘어가서 이런 결과가.. ㅎ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynliQ3wHmXfS",
        "outputId": "d3988fa1-086f-4c93-c50b-06005f714c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[33, ['야', '짱개', '언어', '그냥', '모르', '어도', '되', '거든', '?'], ['짱개', '어도'], ['어도']]\n",
            "\n",
            "[104, ['걔', '네', '이름', '부터', '정', '떨어지', 'ᆷ', 'ㅋㅋ', '평생', '땅', '치', '고', '후회', '하', '었', '으면'], ['정', '떨어지', '땅', '치'], ['정', '으면']]\n",
            "\n",
            "[135, ['안락사', '는', '죽음', '을', '존엄', '하', '게', '맞', '을', '수', '있', '게', '돕', '자는', '거', '이', 'ᆫ데', '.'], ['안락사', '죽음', '맞', '자는'], ['자는']]\n",
            "\n",
            "[171, ['저번', '대선', '홍', '이', '문', '개털', '었', '는데', '?', '탄핵', '때문', '에', '지', 'ᆫ', '거', '이', '야'], ['홍'], ['는데']]\n",
            "\n",
            "[246, ['너', '아이스크림', '먹', '고', '싶', '다며', '나', '가', '배', '민', '으로', '시키', '어', '주', 'ᆯ까', '?'], ['먹', '시키'], ['먹', '배', '시키', '주']]\n",
            "\n",
            "[368, ['나', '그냥', '멍', '때리', '고', '있', '었', '어'], ['멍', '때리'], ['때리']]\n",
            "\n",
            "[413, ['감방', '가', '어서', '도', '또', '사기치', '어', '먹', '을', '궁리', '하', '겠', '지', '점점', '더', '지능', '것', '으로', '크', 'ᆫ', '사기치력', '이', '고', '열심히', '연구', '하', '겠', '구만', '하', '는', '넘', '은', '또', '하', '지', '.'], ['감방', '사기치', '먹', '지능', '크', '넘'], ['어서', '먹', '크']]\n",
            "\n",
            "[417, ['아니야', '장애', '있', '는', '사람', '들', '도', '결혼', '잘', '하', '어서', '아이', '도', '잘', '낳', '을', '수', '있', '게', '하', '어', '주', '어야지'], ['장애', '사람', '들', '잘', '잘', '낳', '어야지'], ['장애', '들', '잘', '어서', '잘', '낳', '주']]\n",
            "\n",
            "[577, ['형국', '을', '알', '어야', '너', '같', '은', '놈', '한테', '도', '짓밟히', '지', '않', '을', '거', '알', '냐'], ['놈'], ['알', '어야', '한테', '알', '냐']]\n",
            "\n",
            "[672, ['저', '놈', '아버지', '가', '일본', '놈', '밑', '에서', '마름질', '하', '었었', '어', '.'], ['놈', '놈', '밑'], ['밑']]\n",
            "\n",
            "[677, ['아', '진짜', '?', '슬슬', '단물', '빠지', 'ᆫ', '듯', '때리', '어', '치', '어라'], ['빠지', '때리', '치', '어라'], ['진짜', '빠지', '때리', '어라']]\n",
            "\n",
            "[716, ['판사', '가족', '이', '당하', '었', '으면', '사형', '감', '이', '지', '.'], ['가족', '당하', '사형', '감'], ['당하', '으면', '감']]\n",
            "\n",
            "[798, ['지지자', '들', '은', '지', 'ᆯ', '때', '도', '조용히', '지', '게', '내버리', '어', '두', '지', 'ᆯ', '않', '네', '.'], ['들', '내버리', '두'], ['들', '때']]\n",
            "\n",
            "[961, ['삼시샛기', '차리', '어', '주', '니까', '고맙', '은', '줄', '도', '모르', '고', '.', '이제', '부터', '너', '가', '하', '어', '보', 'ᆯ래', '?'], ['차리', '줄'], ['주', '니까', '줄', '보']]\n",
            "\n",
            "[974, ['그냥', '좀', '닥치', '고', '게임', '이나', '하', '지', '?'], ['닥치', '이나'], ['좀', '이나']]\n",
            "\n",
            "[1047, ['나', 'ᆫ', '시체', '토막', '을', '보', '면', '잔뜩', '흥분', '되', '더라'], ['시체', '토막'], ['보', '흥분', '더라']]\n",
            "\n",
            "[1204, ['남자', '들', '은', '다', '도살', '시키', '어', '버리', '었', '음', '좋', '겠', '어', '세계', '평화', '는', '남자', '사살', '이', '지', '뭐'], ['들', '도살', '시키', '버리', '좋', '사살'], ['남자', '들', '시키', '버리', '좋', '남자']]\n",
            "\n",
            "[1310, ['1', '단계', '는', '솔직히', '부모', '입김', '아니', '었', '으면', '발끝', '도', '못', '따라가', '었', '다'], ['부모'], ['으면']]\n",
            "\n",
            "[1332, ['나', '는', '일찍', '죽', '을', '만', '하', 'ᆫ', '멍청하', 'ᆫ', '짓', '은', '하', '지', '않', '을래'], ['죽', '멍청하', '짓'], ['죽', '짓']]\n",
            "\n",
            "[1372, ['아', '진짜', '개빡', '치', 'ᆫ다'], ['개빡', '치'], ['진짜']]\n",
            "\n",
            "[1396, ['님', '존예보스', '이', 'ᆫ', '거', '같', '어요', '.'], [], []]\n",
            "\n",
            "[1521, ['걔', '는', '그', '사람', '잘생기', '었', '다고', '지지', '하', '는', '빠돌', '이', '더라'], ['사람', '지지'], ['잘생기', '더라']]\n",
            "\n",
            "[1528, ['뭐', '타투', '하', 'ᆯ', '수', '는', '있', '는데', '저렇', '게', '팔', '덮', '은', '자식', '이랑', '은', '겸상', '도', '하', '지', '말', '어'], ['팔', '자식', '말'], ['는데', '팔', '말']]\n",
            "\n",
            "[1537, ['저런', '미개', '하', 'ᆫ', '것', '들', '을', '우리', '가', '왜', '따라하', 'ᆫ다고', '헛', '소리', '하', '는', '거', '이', '야', '?'], ['미개', '들', '헛', '소리'], ['미개', '들', '우리']]\n",
            "\n",
            "[1697, ['나', '어제', '하체', '헬스', '로', '조지', '었', '더니', '못', '걷', '겠', '어'], ['조지', '걷'], ['하체']]\n",
            "\n",
            "[1810, ['저러', '다', '떨어지', '면', '어쩌', '려고', '그렇', '어', '?', '뒤지', '려면', '지', '혼자', '뒤지', '지'], ['떨어지', '뒤지', '뒤지'], ['저러', '혼자']]\n",
            "\n",
            "[2040, ['이혼', '소송', '지', '어서', '돈', '이나', '줄창', '뜯기', '었', '으면', '좋', '겠', '다', '.'], ['이나', '뜯기', '좋'], ['어서', '돈', '이나', '으면', '좋']]\n",
            "\n",
            "[2143, ['그니까', '걍', '쓰레기', '같', '은', '애', '들', '이', '다'], ['쓰레기', '들'], ['들']]\n",
            "\n",
            "[2203, ['참교육', '가', '어야', '겠', '네'], ['참교육'], ['어야']]\n",
            "\n",
            "[2368, ['아니', '근데', '지', '얘기', '도', '재밌', '으면', '모르', '어', '재미', '도', '디지게', '없', '어', '가지', '고', 'ㅋㅋ'], ['디지게', '가지'], ['으면', '가지']]\n",
            "\n",
            "[2451, ['아', '배', '아프', '어', '요새', '변비', '이', 'ᆫ지', '똥', '이', '안', '나오', '어', 'ㅠㅠ', '미치', '겠', '어'], ['똥', '나오', '미치'], ['배', '변비', '똥']]\n",
            "\n",
            "[2469, ['가만히', '있', '어', '이', '벌레', '이', '야', '.'], ['벌레'], []]\n",
            "\n",
            "[2471, ['방역', '못', '하', '면', '돌멩이', '던지', 'ᆯ', '거', '이', '다'], ['돌멩이', '던지'], ['던지']]\n",
            "\n",
            "[2474, ['진짜', '철딱서니', '가', '없', '어도', '너무', '없', '어'], ['어도'], ['진짜', '어도']]\n",
            "\n",
            "[2564, ['부모', '세대', '에', '는', '자식', '없', '으면', '그냥', '장애인', '취급', '당하', '던', '사회', '이', '었', '는데', '가난', '하', 'ᆫ', '부모', '들', '이', '좋', '어서', '애', '를', '낳', '었', '겠', '냐'], ['부모', '자식', '장애인', '취급', '당하', '사회', '가난', '부모', '들', '좋', '낳'], ['으면', '당하', '는데', '들', '좋', '어서', '낳', '냐']]\n",
            "\n",
            "[2657, ['진짜', '쓰', 'ᆯ', '때', '마다', '개', '빡', '치', '네'], ['쓰', '개', '빡', '치'], ['진짜', '쓰', '때', '마다', '빡']]\n",
            "\n",
            "[2742, ['도련님', '못', '하', '겠', '으면', '아이', '삼촌', '이', '라고', '하', '어라'], ['어라'], ['으면', '어라']]\n",
            "\n",
            "[2752, ['그냥', '개취', '같', '더라고'], [], []]\n",
            "\n",
            "[2909, ['야', '걔', '네', '도', '나름', '개과천선', '하', '어서', '열심히', '하', 'ᆫ', '거', '이', '야'], [], ['어서']]\n",
            "\n",
            "[2914, ['너', '진짜', '핑프', '같', '어'], [], ['진짜']]\n",
            "\n",
            "[3027, ['나', '는', '학교', '가', 'ᆯ', '준비', '도', '하', '어야', '하', '고', '보통', '은', '엄마', '가', '차리', '어', '주', '지', '않', '어', '너', '는', '니', '가', '차리', '어', '먹', '냐', '??'], ['엄마', '차리', '차리', '먹'], ['어야', '엄마', '주', '먹', '냐']]\n",
            "\n",
            "[3097, ['여보', '식', '세기', '한', '대', '사', '자', '설거지', '하', '기', '힘들', '어', 'ㅜㅜ'], ['식', '대'], ['대', '사', '자', '설거지']]\n",
            "\n",
            "[3167, ['덜미', '만', '잡히', 'ᆫ', '것', '이', '아니', '라', '모가지', '까지', '잡히', 'ᆫ', '거', '이', 'ᆸ니다', '.'], ['모가지'], ['까지']]\n",
            "\n",
            "[3238, ['너', '지금', '부리', '또', '사', '어', '오', 'ᆫ', '거', '이', '고', '기', '말', '고', '뭐', '가', '들', '어', '있', '는', '거', '이', '야', '?'], ['부리', '말', '들'], ['사', '말', '들']]\n",
            "\n",
            "[3343, ['아무리', '국선', '변호사', '이', '라도', '양심', '이', '있', '으면', '죄', '를', '인정', '하', 'ᆫ다고', '하', '어야지', ',', '저런', '애', '들', '이', '사회', '에', '나오', '어서', '개', '차', '반', '으로', '살', 'ᆯ', '거', '뻔히', '알', '면서', '.'], ['라도', '양심', '죄', '인정', '어야지', '들', '사회', '나오', '개', '반', '살', '면서'], ['으면', '들', '어서', '살', '알']]\n",
            "\n",
            "[3393, ['맞', '어', '.', '나', '는', '살인', '이랑', '동급', '이', '라고', '보', '어'], ['맞', '살인'], ['보']]\n",
            "\n",
            "[3477, ['사형', '은', '시키', '면', '안', '되', '어'], ['사형', '시키'], ['시키']]\n",
            "\n",
            "[3600, ['1000', '원', '에', '반찬', '세', '개', '랑', '밥', ',', '국', '나오', '면', '혜자', '아니', 'ᆷ', '?'], ['개', '밥', '국', '나오'], ['세', '밥']]\n",
            "\n",
            "[3626, ['나', '도', '흑인', '으로', '살', 'ᆯ', '바', '에', 'ᆫ', '그냥', '다음', '생', '도박', '하', 'ᆯ', '듯'], ['흑인', '살', '도박'], ['흑인', '살']]\n",
            "\n",
            "[3744, ['남', '의', '나라', '에서', '돈', '받', '어', '가', '는', '주제', '에', '감', '내놔라', '배', '내', '어', '놓', '어라', '하', '네', '.'], ['주제', '감', '놓', '어라'], ['돈', '받', '감', '배', '놓', '어라']]\n",
            "\n",
            "[3746, ['장사', '하', '는', '사람', '들', '은', '물건', '안', '팔리', '면', '가게', '다', '불', '지르', '어야', '하', '나', '?'], ['사람', '들', '불', '지르'], ['장사', '들', '물건', '불', '지르', '어야']]\n",
            "\n",
            "[3747, ['성형', '괴물', '길거리', '에', '좀', '안', '나오', '었', '으면', '좋', '겠', '다'], ['괴물', '길거리', '나오', '좋'], ['좀', '으면', '좋']]\n",
            "\n",
            "[3789, ['저', '예쁘', 'ᆫ', '아가', '를', '어떻', '게', '굶기', '어', '죽이', '냐'], ['예쁘', '아가', '굶기', '죽이'], ['예쁘', '어떻', '죽이', '냐']]\n",
            "\n",
            "[3849, ['나', '는', '말', '하', '는', '감자', '이', '야', '.'], ['말', '감자'], ['말']]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. 우선, 폭언으로 잘못 판별된 문장들이 사전의 어느 부분이랑 매칭되는지 확인해볼까요?\n",
        "# 이때, 사전은 폭언, 성희롱 사전 모두 사용하셔야 됩니다 --> 실제 시스템에서 두개다 매칭을 진행할거라서 ㅇㅇ\n",
        "\n",
        "for rows in moral_abuse_morphs:\n",
        "  abuse_matched = list()\n",
        "  sexual_matched = list()\n",
        "\n",
        "  for morph in rows[1]:\n",
        "    if morph in abuse_list:\n",
        "      abuse_matched.append(morph)\n",
        "    if morph in sexual_list:\n",
        "      sexual_matched.append(morph)\n",
        "\n",
        "  rows.append(abuse_matched)\n",
        "  rows.append(sexual_matched)\n",
        "\n",
        "for rows in moral_abuse_morphs:\n",
        "  print(rows)\n",
        "  print()\n",
        "\n",
        "# 폭언으로 잘못 분류된 문장 모두에 대해 (test_set 인덱스, 문장 형태소, 폭언 사전 매칭 형태소, 성희롱 사전 매칭 형태소)\n",
        "# 보면 1396이랑,2752 인덱스는 어떤 사전이랑도 매칭되는 게 없잖아요. 바로 해당 문장들이 사전을 적용하게 되면, 매칭이 되는게 없어서 바로 일반 문장으로 판단되는 문장들입니다!\n",
        "# 제가 말했었죠. 그 사전 적용하면 딱 2문장만 걸러진다고요. 그게 바로 이 두문장입니다!!\n",
        "# 그러니까, 지금 저희가 하는 일이. 이 두문장 말고도 사전에 매칭되는 문장이 없게끔 문장, 사전을 각색하는 일입니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "8dHmjgxdopUM"
      },
      "outputs": [],
      "source": [
        "# 4. 사전 각색 방법\n",
        "\n",
        "# 위에 문장 중에서, 171 인덱스 문장을 바탕으로 사전을 각색하는게 좋아보여요\n",
        "# 폭언 사전에서는 '홍', 성희롱 사전에서는 '는데'만 매칭되었거든요\n",
        "# 그럼 해당 키워드를 제거하기 전에, 기존에 '홍', '는데' 키워드만 가지고 있던 폭언, 성희롱 문장이 있을 수도 있어요\n",
        "# 그런 문장이 있는지 확인해봐여\n",
        "\n",
        "def isOnlyThatKeyword(keyword):\n",
        "  res = list()\n",
        "\n",
        "  for rows in test_set:\n",
        "    if rows[1] == 1 or rows[1] == 2:  \n",
        "      temp_all = list()\n",
        "      temp_abuse = list()\n",
        "      temp_sexual = list()\n",
        "      morphs = kiwi.tokenize(rows[0])\n",
        "\n",
        "      flag = False\n",
        "      for morph in morphs:\n",
        "        if morph.form == keyword:\n",
        "          flag =True\n",
        "      \n",
        "      if flag == True:\n",
        "        for morph in morphs:\n",
        "          temp_all.append(morph.form)\n",
        "\n",
        "          if morph.form in abuse_list:\n",
        "            temp_abuse.append(morph.form)\n",
        "          if morph.form in sexual_list:\n",
        "            temp_sexual.append(morph.form)\n",
        "        \n",
        "      if len(temp_abuse) + len(temp_sexual) == 1:\n",
        "        res.append([temp_all, temp_abuse, temp_sexual])\n",
        "      \n",
        "\n",
        "  return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWPguDAru045",
        "outputId": "aec71d7e-9a9c-4163-f4ec-e25a0d3a1653"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "isOnlyThatKeyword('홍')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDSuoOgowPW_",
        "outputId": "e6787a06-1a7c-42aa-d537-63edcab97865"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "isOnlyThatKeyword('는데') # 홍, 는데만으로 매칭되는 폭언, 성희롱 문장은 업네여!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfiBc_ksyA48",
        "outputId": "b4ab3c99-86db-4117-c20e-9f4d3e1512a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['저번대선 홍이 문 개털었는데? 탄핵때문에 진거야', 0]\n"
          ]
        }
      ],
      "source": [
        "# 홍, 는데 모두 각 키워드만으로 매칭되는 실제 폭언, 성희롱 문장이 없다네요\n",
        "# 즉, 사전에서 해당 두 키워드를 즉시 삭제해도, 무관하다는 뜻입니다\n",
        "# 이렇게 경우의 수를 전부 계산해서 사전 csv 에서 키워드를 삭제하면 됩니다\n",
        "# 단, 유의할 점은 이렇게 사전에서 키워드를 삭제하면, 다른 문장 검증시 갱신된 사전을 바탕으로 진행해야 합니다 --> 그냥 하면 미세한 확률로 에러가 날 수도! ㅎ\n",
        "\n",
        "# 그래서 두번째 방법을 알려드립니다\n",
        "\n",
        "print(test_set[171])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "_JKsXtKOyu0R"
      },
      "outputs": [],
      "source": [
        "# 5. 문장 각색\n",
        "\n",
        "# 예를 들어, 위의 171번 문장에서 문제가 되는게, [홍, 는데] 두개니까\n",
        "example = \"저번대선 김이 문 이김, 탄핵때문에 진거야\"\n",
        "# 이런식으로 홍,는데 를 없애서 회피하듯이 문장을 다시 짜는 겁니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "q2Zq5nKQzI7e"
      },
      "outputs": [],
      "source": [
        "# 해당 문장은 문제가 없는지 검증해볼까요\n",
        "\n",
        "def validation(sentence):\n",
        "  morphs = kiwi.tokenize(example)\n",
        "  flag = False\n",
        "  for morph in morphs:\n",
        "    if morph.form in abuse_list or morph.form in sexual_list:\n",
        "      print(morph.form)\n",
        "      flag = True\n",
        "  \n",
        "  if flag == False:\n",
        "    print(\"문장 각색 완료\")\n",
        "  else:\n",
        "    print(\"위의 키워드가 문제가 됩니다. 문장을 다시 각색하세요!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXQJ_DbrzjoQ",
        "outputId": "08e4a16b-a630-44dc-a068-83f6fd100111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "김\n",
            "이기\n",
            "위의 키워드가 문제가 됩니다. 문장을 다시 각색하세요!\n"
          ]
        }
      ],
      "source": [
        "validation(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t01Q7PBC0U57",
        "outputId": "77be99e1-3ffc-4633-e2df-cf397fe479c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 각색 완료\n"
          ]
        }
      ],
      "source": [
        "# 각색 결과 또 문제가 생기네여 ㅋㅋㅋㅋㅋㅋㅋㅋ\n",
        "# 그럼 이번에는 이렇게 바꿀게여\n",
        "example = \"저번대선 윤이 문 개텀, 탄핵때문에 진거야\"\n",
        "validation(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "r4sTOKN80inA"
      },
      "outputs": [],
      "source": [
        "# 이번에는 문제가 없네여!\n",
        "# 그럼 해당 문장을 csv에서 바꿔주면 됩니다!\n",
        "\n",
        "# ㅎㅎ.. 복잡하죠 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n",
        "# 사전 각색 vs 문장 각색 비교하면, 아무래도 문장 각색이 신경쓸게 덜합니다  \n",
        "# 사전 각색이 귀찮은 이유는 사전에서 키워드를 제거하고 갱신된 사전을 다시 코랩으로 로드하고 다음 문장을 작업해야하기 때문입니다 (갱신 안하고 작업하면, 정확한 산출이 불가)\n",
        "\n",
        "# 그래서 왠만하면 적절하게 문장을 각색하고 검증하는 방식을 이용하면 됩니다!\n",
        "# 하지만, 제가 사용한 예시 문장처럼 '는데' 이런 말도안되는 키워드가 문제가 되면, 아무래도 사전 자체에서 삭제하는 편이 좋겠죠?\n",
        "# --> 판단은 알아서!, 사전 각색이 귀찮으면, 전부 문장 각색으로 하셔도 됩니다~ --> 제가 봤을 때는 '는데'만 삭제하고 이후에는 그냥 문장 각색으로 빠르게 해도 될 것 같네요\n",
        "\n",
        "# 위의 알고리즘에서 3번부터는 폭언으로 잘 못 판별한 문장으로만 진행 중입니다\n",
        "# 그러니까 성희롱으로 잘 못 판별한 문장에서도 각색이 필요해요!\n",
        "# 성희롱 코드는 아래에서 확인!\n",
        "\n",
        "# 이렇게 30문장 정도만 추가로 거르면 좋을 것 같아요! 고생하세여! ㅎ  (죄송해요ㅜ 많으면 많을 수록 좋을 것 같아요..ㅎ)\n",
        "# 아 각색 과정에서, 사전을 수정했으면, 어떤 키워드를 수정했는지\n",
        "# 문장을 수정했으면 어떤 문장을 수정했는지 인덱스 정보를 기록해두세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuK7vOi62o5R",
        "outputId": "661fa9c5-3a6c-4c2f-9511-ef4b519da5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25, ['성관계', '끝나', '었', '는데', '도', '계속', '이쁘', '다고', '하', '어', '주', '고', '이름', '이랑', '전화', '번호', '도', '알리', '어', '주', '네', '.'], ['알리'], ['성관계', '는데', '주', '주']]\n",
            "\n",
            "[142, ['애견', '카페', '가', '었', '는데', '나', '의', '옆', '에서', '강아지', '들', '짝짓기', '하', 'ᆷ', '..'], ['들'], ['는데', '옆', '들', '짝짓기']]\n",
            "\n",
            "[155, ['새', '거', '도', '아니', 'ᆫ', '스타킹', '으로', '포장', '되', '어', '있', '음'], ['새'], ['스타킹']]\n",
            "\n",
            "[159, ['러시아', '여자', '들', '이', '리얼', '예쁘', 'ᆷ'], ['여자', '들', '예쁘'], ['여자', '들', '예쁘']]\n",
            "\n",
            "[285, ['잘', '찍', '고', '오', '었', '지', '~', '근데', '가슴', '이', '작', '어서', '곤란', '하', '었', '음'], ['잘', '찍', '가슴', '작'], ['잘', '찍', '가슴', '작', '어서']]\n",
            "\n",
            "[301, ['무슨', '대단', '하', 'ᆫ', '집', '아들', '이', 'ᆫ', '줄', '알', '었', '더니', '남자', '개털', '이', '구만'], ['아들', '줄'], ['무슨', '줄', '알', '남자']]\n",
            "\n",
            "[569, ['넘', '귀엽', '지', '않', '냐', '??', '저', '정도', '이', '면', '강아지', '탈', '을', '쓰', 'ᆫ', '사람', '아니', 'ᆯ까', 'ㅋㅋ'], ['넘', '탈', '쓰', '사람'], ['냐', '탈', '쓰']]\n",
            "\n",
            "[1091, ['ㄹㅇ', '거기', '에', '돈', '쓰', '는', '아재', '가', '얼마나', '많', '은데'], ['거기', '쓰', '아재', '많'], ['거기', '돈', '쓰', '많']]\n",
            "\n",
            "[1102, ['평소', '에', '덜렁대', '었', '잖아', '.', '너', '도', '알', '면서'], ['면서'], ['알']]\n",
            "\n",
            "[1168, ['2', '년', '동안', '구르', '고', '박', '고', '얻', '어', '터지', 'ᆫ', '적', '없', '으면', '입', '딱', '다물', '어라'], ['년', '구르', '박', '터지', '적', '입', '딱', '다물', '어라'], ['년', '구르', '박', '터지', '적', '으면', '입', '딱', '어라']]\n",
            "\n",
            "[1219, ['달고나', '가', '뭐', '가', '맛있', '다고', '난리', '들', '이', '야', ';'], ['난리', '들'], ['맛있', '들']]\n",
            "\n",
            "[1389, ['쟤', '가', '아직', '도', '뮤지컬', '쪽', '에서', '는', '몸값', '탑', '이', '야', '.'], [], ['몸값', '탑']]\n",
            "\n",
            "[1436, ['청국장', '좋아하', '는', '애', '들', '보', '면', '아재', '입맛', '이', 'ᆫ', '거', '같', '은데', '좀', '그렇', '다'], ['들', '아재'], ['들', '보', '좀']]\n",
            "\n",
            "[1642, ['우리', '겜방', '가', '어서', '겜', '한판', '만', '하', '자'], [], ['우리', '어서', '한판', '자']]\n",
            "\n",
            "[1750, ['환자', '덮치', '는', '의사', '가', '의사', '이', '냐'], ['환자'], ['덮치', '냐']]\n",
            "\n",
            "[1761, ['막', '줍', '다가', '잡히', '어', '가', '는', '거', '아니', '야', '?', 'ㅋㅋ'], ['막'], ['막', '다가']]\n",
            "\n",
            "[2078, ['머', '가', '있', '어야', '적을거잔아요', '나', 'ᆫ', '장점', '없', '어요', '.'], ['머'], ['머', '어야']]\n",
            "\n",
            "[2097, ['가슴', '크', 'ᆫ', '여자', '들', '크로스', '백', '메', '는', '거'], ['가슴', '크', '여자', '들', '메'], ['가슴', '크', '여자', '들']]\n",
            "\n",
            "[2115, ['이거', '보', '면', '배꼽', '빠지', 'ᆷ'], ['빠지'], ['보', '배꼽', '빠지']]\n",
            "\n",
            "[2267, ['분명히', '해', '쨍쨍', '하', 'ᆯ', '거', '이', '라', '하', '었었', '는데'], [], ['해', '는데']]\n",
            "\n",
            "[2758, ['우리', '어리', 'ᆯ', '때', '는', '피카츄', '진짜', '많이', '먹', '었', '는데', 'ㅎ'], ['어리', '먹'], ['우리', '어리', '때', '진짜', '먹', '는데']]\n",
            "\n",
            "[2765, ['니', '딸', '도', '아니', 'ᆫ데', '40', '대', '를', '만나', '어', '들', 'ᆫ', '50', '대', '를', '만나', '든', '왜', '열', '폭', '이', '냐'], ['딸', '대', '들', '대', '폭'], ['딸', '대', '만나', '들', '대', '만나', '폭', '냐']]\n",
            "\n",
            "[2807, ['우리', '나라', '는', '아직', '야매', '가', '많이', '서', '글', '치', '멋지', 'ᆫ데', '?'], ['서', '치'], ['우리', '서', '글']]\n",
            "\n",
            "[3074, ['저쪽', '은', '엉덩이', '수술', '많이', '하', 'ᆫ대', '요'], [], ['엉덩이', '수술']]\n",
            "\n",
            "[3177, ['왜', '화', '를', '내', '어', '~', '몰카', '야', 'ㅋㅋ'], ['화', '몰카'], ['화', '몰카']]\n",
            "\n",
            "[3341, ['아', '뭐', '이', '라는', '거', '이', '야', '얘', '네', '츄르', '에', '환장', '하', '어'], ['환장'], []]\n",
            "\n",
            "[3347, ['여자', '친구', '를', '사귀', '면', '헤프', '게', '놀', '는', '거', '이', '야', '?'], ['여자', '사귀'], ['여자', '친구', '헤프', '놀']]\n",
            "\n",
            "[3375, ['남', '의', '집', '복도', '계단', '에서', '어떤', '놈', '이', '딸', '치', '다가', 'cctv', '에', '찍히', '었', '대'], ['놈', '딸', '치', '찍히', '대'], ['딸', '다가', '대']]\n",
            "\n",
            "[3388, ['누구', '가', '자꾸', '나', '의', '속옷', '훔치', '어', '가', 'ᆫ다', '누구', '이', '냐'], [], ['속옷', '냐']]\n",
            "\n",
            "[3532, ['그냥', '다', '밈', '이', '지', '설마', '진짜', '이', '겠', '냐'], [], ['진짜', '냐']]\n",
            "\n",
            "[3534, ['나', 'ᆫ', '단벌', '숙녀', '이', '야', '그러', 'ᆯ', '수', '있', '자', 'ᆫ', '아'], [], ['자']]\n",
            "\n",
            "[3749, ['사자', '의', '저', '멋지', 'ᆫ', '갈기', '들', '이', '더', '멋지', '게', '보이', '는', '거', '같', '애'], ['사자', '갈기', '들', '보이'], ['갈기', '들', '보이']]\n",
            "\n",
            "[3854, ['오키나와', '여행', '가', '었', '을', '때', '까마귀', '가', '가방', '지퍼', '열', '고', '빵', '꺼내', '어', '가', '더라', 'ㅋㅋ'], ['빵', '꺼내'], ['때', '빵', '더라']]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 부록. 성희롱으로 잘 못 판별한 문장\n",
        "\n",
        "for rows in moral_sexual_morphs:\n",
        "  abuse_matched = list()\n",
        "  sexual_matched = list()\n",
        "\n",
        "  for morph in rows[1]:\n",
        "    if morph in abuse_list:\n",
        "      abuse_matched.append(morph)\n",
        "    if morph in sexual_list:\n",
        "      sexual_matched.append(morph)\n",
        "\n",
        "  rows.append(abuse_matched)\n",
        "  rows.append(sexual_matched)\n",
        "\n",
        "for rows in moral_sexual_morphs:\n",
        "  print(rows)\n",
        "  print()\n",
        "\n",
        "# 성희롱으로 잘못 분류된 문장 모두에 대해 (test_set 인덱스, 문장 형태소, 폭언 사전 매칭 형태소, 성희롱 사전 매칭 형태소)\n",
        "# 제가 봤을 때는 1102, 2078, 2267 요런 문장들이 각색하기 편해 보이네여"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "U7QtMgW5chSj",
        "Kjhhqi-DeDXQ",
        "FXGKWs2GgBxG",
        "ZDFnsU6tgty1"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
